{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2024-2025 - UMONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VU95foPgSR7C"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will experiment with two regularization methods, **ridge** and **LASSO**. In the regression setting, given a training dataset $\\mathcal{D} = \\{(x_i,y_i)\\}_{i=1}^n$, recall from the course that ridge regression aims to find the coefficients $\\beta^\\text{R}$ that mimimize the following optimization problem:\n",
    "\n",
    "$$\\beta^\\text{R} = \\underset{\\beta}{\\text{argmin}} \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2,$$\n",
    "\n",
    "where $\\lambda$ is a hyper-parameter controlling the amount of shrinkage applied to the coefficients $\\beta_j$. On the other hand, the LASSO regression coefficients are obtained from the following optimization problem:\n",
    "\n",
    "$$ \\beta^\\text{L} = \\underset{\\beta}{\\text{argmin}} \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|.$$ \n",
    "\n",
    "In the binary classification setting, where $\\mathcal{Y} = \\{0,1\\}$, ridge logistic regression aims instead to solve the following optimization problem:\n",
    "\n",
    "$$\\beta^\\text{R} = \\underset{\\beta}{\\text{argmin}}\\left[-\\text{log}~\\mathcal{L(\\boldsymbol{\\beta};\\mathcal{D})} + \\lambda \\sum_{j=1}^p \\beta_j^2\\right],$$\n",
    "\n",
    "where $-\\text{log}~\\mathcal{L(\\boldsymbol{\\beta};\\mathcal{D})}$ is the negative conditional log-likelihood, i.e.,\n",
    "\n",
    "$$-\\text{log}~\\mathcal{L(\\boldsymbol{\\beta};\\mathcal{D})} = -\\frac{1}{n} \\sum_{i=1}^n y_i\\text{log}~p(y_i=1\\mid\\boldsymbol{x}_i; \\boldsymbol{\\beta}) + (1-y_i)\\text{log}~p(y_i=0\\mid\\boldsymbol{x}_i;\\boldsymbol{\\beta}),$$\n",
    "\n",
    "with ${p(y_i=1\\mid\\boldsymbol{x}_i;\\boldsymbol{\\beta}) = \\frac{e^{\\boldsymbol{\\beta}^T \\boldsymbol{x}_i}}{1 + e^{\\boldsymbol{\\beta}^T \\boldsymbol{x}_i}}}$ being a logistic regression classifier where $\\boldsymbol{\\beta}, \\boldsymbol{x}_i \\in \\mathbb{R}^p$. Alternatively, LASSO logistic regression aims to minimize the following objective:\n",
    "\n",
    "$$\\beta^\\text{L} = \\underset{\\beta}{\\text{argmin}}\\left[-\\text{log}~\\mathcal{L(\\boldsymbol{\\beta};\\mathcal{D})} + \\lambda \\sum_{j=1}^p |\\beta_j|\\right].$$\n",
    "\n",
    "The purpose of the lab is to better understand the influence of $\\lambda$ on the fitted models, in both the regression and classification settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaQPVA_VSZo4"
   },
   "source": [
    "**Import the necessary libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1X5-IyjYbGFK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Lasso, LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWYl55GYSgyc"
   },
   "source": [
    "**Use the code snippet below to generate some data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUOiWN0wbQl4"
   },
   "outputs": [],
   "source": [
    "def generate(n_samples:int=100 ,n_features:int=5,n_informatives:int=5,coef:bool=True):\n",
    "    X, y, coef = datasets.make_regression(\n",
    "    n_samples=n_samples,  # number of samples\n",
    "    n_features=n_features,  # number of features\n",
    "    n_informative=n_informatives,  # number of useful features\n",
    "    noise=10,  # standard deviation of the gaussian noise\n",
    "    coef=coef,  # true coefficient used to generated the data\n",
    "    random_state=0,  # always create the same dataset\n",
    ")\n",
    "    return X , y , coef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JTVEaod7Sssr"
   },
   "source": [
    "**1) Fit a ridge regression to the data for $\\lambda$ varying between $1$ and $2000$ by steps of $1$, and plot the evolution of the coefficients as a function of $\\lambda$. What do you observe? Do the same operation for LASSO regression with $\\lambda$ varying between $1$ and $100$.**\n",
    "\n",
    "You can use the methods `Ridge()` and `Lasso()` from the scikit-learn library. Note that the hyperparameter we called $\\lambda$ is called `alpha` in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5HwQ2kZWcEZ0",
    "outputId": "62dc97c2-a46f-4761-b416-f9957302da0d"
   },
   "outputs": [],
   "source": [
    "# creation de\n",
    "modelsR = np.array([ Lasso(alpha=i) for i in range(1,2000)] )\n",
    "modelsL = np.array([ Lasso(alpha=i) for i in range(1,2000)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "NEZ8np5heY88",
    "outputId": "716517ee-75d1-44fa-ff7a-2894db6ccb33"
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bO0ylaalT-w2"
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aOvMOvEVUQ4S"
   },
   "source": [
    "**2) Reuse the code snippet above to generate $100$ samples with $90$ features, with only $2$ being informative on the response $y$. Split your dataset on a train and a test split using a $80/20$ partition. For increasing values of $\\lambda$ between $0.1$ and $10$ (by steps of $0.1$), fit a ridge regression model on the training data and plot its MSE as a function of $\\lambda$. What do you observe? Do the same for LASSO regression.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "Ymh0bFUQj8x9",
    "outputId": "57070bd5-d1b3-4c00-8e39-370b1c968a1b"
   },
   "outputs": [],
   "source": [
    "X,y,coef = generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "6bGMyoVKBH3X",
    "outputId": "75abb29c-3bdb-44e0-d1ef-8897c2d21cd8"
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U0GZ-QoXVzB9"
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UHd9xeGjZJ5g"
   },
   "source": [
    "**3) Use the code snippet below to simulate some data. Then, perform the following steps:**\n",
    "- **Split your dataset into training and test sets following a $80/20$ partition.**\n",
    "- **For values of $\\lambda$ in $[0,10]$, perform a GridSearch cross-validation with `cv = 10` to identify the best value of $\\lambda$ for both ridge and LASSO regression.**\n",
    "    - **You will need the `GridSearchCV()` method of scikit-learn for this step. Use the mean squared error as selection metric.**\n",
    "    - **What are the best values $\\lambda^\\text{R}$ and $\\lambda^\\text{L}$ for ridge and LASSO, respectively?**\n",
    "- **Using these best values of $\\lambda^\\text{R}$ and $\\lambda^\\text{L}$, refit the models to the full training set.**\n",
    "- **Evaluate the training and test MSE using the fitted models, and compare it to the training and test MSE of a linear regression model fitted on all predictors.**\n",
    "- **How many coefficients are non-zero for the LASSO regression?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coef = datasets.make_regression(\n",
    "    n_samples=500,  # number of samples\n",
    "    n_features=350,  # number of features\n",
    "    n_informative=2,  # number of useful features\n",
    "    noise=5,  # bias and standard deviation of the guassian noise\n",
    "    coef=True,  # true coefficient used to generated the data\n",
    "    random_state=1,  # always create the same dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the code snippet below to generate a classification dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(\n",
    "    n_samples=200,  # number of samples\n",
    "    n_features=30,  # number of features\n",
    "    n_informative=5,  # number of useful features\n",
    "    random_state=0,\n",
    "    weights=[0.8, 0.2],  # Proportion of samples per class.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Using the generated dataset above, apply the following steps:**\n",
    "- **Split your dataset into training and test sets following a $80/20$ partition.**\n",
    "- **For values of $\\lambda$ in $[0,10]$, perform a GridSearch cross-validation with `cv = 10` to identify the best value of $\\lambda$ for both the ridge and LASSO logistic regression classifiers.**\n",
    "    - **Check the arguments `penalty` and `C` of the `LogisticRegression()` method to define ridge and LASSO classifiers. Select `saga` as solver for ridge and `liblinear` as solver for LASSO.**\n",
    "    - **You will need the `GridSearchCV()` method of scikit-learn for this step. Use the accuracy as selection metric.**\n",
    "    - **What are the best values $\\lambda^\\text{R}$ and $\\lambda^\\text{L}$ for ridge and LASSO, respectively?**\n",
    "- **Using these best values of $\\lambda^\\text{R}$ and $\\lambda^\\text{L}$, refit the models to the full training set.**\n",
    "- **Evaluate the training and test accuracy using the fitted models, and compare it to the training and test accuracy of a logistic regression model fitted on all predictors.**\n",
    "    - **Use the `saga` solver for the logistic regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=0, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the regression setting, we mitigate the overfitting phenomenon using ridge and LASSO logistic regression, which translates into a significant increase in test accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) For the logistic regression, ridge logistic regression and LASSO logistic regression, do:** \n",
    "- **Plot the confusion matrix of the test predictions. You may use the `confusion_matrix()` and `ConfusionMatrixDisplay()` methods.**\n",
    "- **Compute the True Positive Rate, False Positive Rate, True Negative Rate, False Negative Rate, and Precision of the models on the test set.**\n",
    "    - **You can access the necessary quantities out of the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an imbalanced dataset, with only $20\\%$ of observations in the test set belonging to the positive class. Let us say our classifier only classifies observations to the negative class. In this case, the accuracy would be: $\\text{Acc} = \\frac{TP + TN}{N + P} = \\frac{0 + 32}{40} = 0.8$. This means that, even if the model only predicts the $0$ class, we get an accuracy of $80\\%$, which can be highly misleading regarding the true performance of the model. The confusion matrix can give more insight regarding what is really happening behind the scenes. \n",
    "\n",
    "$TPR = \\frac{TP}{TP + FN}$\n",
    "* Amongst all the observations belonging the positive class, how many are correctly classified as positive. \n",
    "\n",
    "$FPR = \\frac{FP}{TN + FP}$ \n",
    "* Amongst all the observations belonging to the negative class, how many are wrongly classified as positive.\n",
    "\n",
    "$TNR = \\frac{TN}{TN + FP}$\n",
    "* Amongst all the observations belonging to the negative class, how many are correctly classified as negative. \n",
    "\n",
    "$FNR = \\frac{FN}{FN + TP}$\n",
    "* Amongst all the observations belonging to the positive class, how many are wrongly classified as negative.\n",
    "\n",
    "$\\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "* Amongst all the observations that are classified by the model as positive, \n",
    "how many are actually positive. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) For the logistic regression, ridge logistic regression, and LASSO logistic regression, do:**\n",
    "- **Plot the ROC curve of the test predictions, and show the AUROC (Area Under the ROC curve) on the same figure.**\n",
    "    - **Check the methods `roc_curve()` and `roc_auc_score()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab14_solutions",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
