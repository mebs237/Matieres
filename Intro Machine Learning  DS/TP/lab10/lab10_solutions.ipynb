{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2024-2025 - UMONS\n",
    "\n",
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will experiment with multi-class classification. We will consider several models.\n",
    "We will be using the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset, which contains several attributes of white wines.\n",
    "Each observation is associated to a rating between $0$ and $10$ that will be the label of our classification task.\n",
    "\n",
    "The columns of the dataframe contain the following information:\n",
    "* `fixed_acidity`: amount of tartaric acid in $\\text{g}/\\text{dm}^3$,\n",
    "* `volatile_acidity`: amount of acetic acid in $\\text{g}/\\text{dm}^3$,\n",
    "* `citric_acid`: amount of citric acid in $\\text{g}/\\text{dm}^3$,\n",
    "* `residual_sugar`: amount of remaining sugar after fermentation stops in $\\text{g}/\\text{l}$,\n",
    "* `chlorides`: amount of salt in wine,\n",
    "* `free_sulfur_dioxide`: amount of free $\\text{SO}_2$,\n",
    "* `total_sulfur_dioxide`: amount of free and bound forms of $\\text{SO}_2$,\n",
    "* `density`: density of the wine,\n",
    "* `pH`: pH level of the wine on a scale from $0$ to $14$,\n",
    "* `sulphates`: amount of sulphates,\n",
    "* `alcohol`: the percent of alcohol content,\n",
    "* `quality`: quality of the wine (score between $0$ and $10$)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the necessary libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,\n",
    "                             confusion_matrix, log_loss, classification_report)\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import warnings\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We load the dataset `wine.csv`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wine.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Check the properties of this dataset (length, types, missing values).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We predict the target `quality` from all other features. We split the dataset into a training and test set following a $80/20$ partition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = 'quality'\n",
    "X = df.drop(ylabel, axis=1)\n",
    "y = df[ylabel]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, shuffle=True, random_state=0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Look at the distribution of the variable `quality` in the training set using `sns.countplot`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2.5))\n",
    "sns.countplot(x=y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) For each continuous feature, we plot a boxplot of this feature grouped by label values. Use the `sns.boxenplot` function of the `seaborn` library. Which features seem to be the most useful to predict the label `quality`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(10, 8), sharex=True)\n",
    "axes = axes.flatten()\n",
    "for column, axis in zip(X_train.columns, axes):\n",
    "    sns.boxenplot(x=y_train, y=X_train[column], ax=axis)\n",
    "axes[-1].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "source": [
    "Based on the boxplots, alcohol, density, and pH seem to be the most useful features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Plot the pairwise relationship of the most useful features using the function `sns.pairplot`. Plot a different color according to the value of the variable `quality` using the `hue` parameter. What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "sns.pairplot(pd.concat([X_train[['alcohol', 'density', 'pH']], y_train], axis=1), hue='quality');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "source": [
    "We notice that there is a negative correlation between the variables alcohol and density. There does not seem to be an obvious correlation between the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Define a simple pipeline where you first scale the data with `StandardScaler` to have zero mean and unit variance followed by a (linear) logistic regression. Then, fit the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) One of the most useful tools to diagnose a classification model is the confusion matrix. Print it using `confusion_matrix` and `ConfusionMatrixDisplay`.**\n",
    "\n",
    "The size of the matrix is $n \\times n$, where $n$ is the number of classes. Each row represents the instances in an actual class, while each column represents the instances in a predicted class. A cell $i, j$ represents the number of instances of class $i$ that were predicted as class $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) From the confusion matrix, several performance metrics can be calculated for each class, as well as overall metrics. Using the function `classification_report`, generate a report of these different metrics. Use the argument `zero_division=0` to avoid warnings.**\n",
    "\n",
    "Here is what each of these terms represents:\n",
    "\n",
    "1. **Precision** (also known as *positive predicted value*): This is the ratio of correctly predicted positive observations to the total predicted positives. It is an indicator of the accuracy of the positive predictions. For class $i$, precision is calculated as\n",
    "   $$\n",
    "   \\text{Precision}_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FP}_i},\n",
    "   $$\n",
    "   where $\\text{TP}_i$ are the true positives for class $i$ and $\\text{FP}_i$ are the false positives for class $i$.\n",
    "\n",
    "2. **Recall** (also known as *sensitivity* or *true positive rate*): This is the ratio of correctly predicted positive observations to all observations in the actual class. It shows how well the model can find all the positive samples. For class $i$, recall is calculated as\n",
    "   $$\n",
    "   \\text{Recall}_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FN}_i},\n",
    "   $$\n",
    "   where $\\text{FN}_i$ are the false negatives for class $i$.\n",
    "\n",
    "3. **$F_1$ Score**: This is the harmonic mean of precision and recall. Therefore, this score takes both false positives and false negatives into account. It is particularly useful when the class distribution is uneven. $F_1$ score is calculated as\n",
    "   $$\n",
    "   \\text{$F_1$ Score}_i = \\frac{2}{\\frac{1}{\\text{Precision}_i} + \\frac{1}{\\text{Recall}_i}} = 2 \\times \\frac{\\text{Precision}_i \\times \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i},\n",
    "   $$\n",
    "   and is between $0$ and $1$ ($1$ means that all predictions are correct, and $0$ means that there is no correct prediction).\n",
    "\n",
    "4. **Support**: This is the number of actual occurrences of the class in the specified dataset. It does not reflect the model's performance but is very useful for determining the significance of the classification metrics.\n",
    "\n",
    "These metrics can be averaged to obtain:\n",
    "- **Macro average**: This is the average of the precision, recall, and $F_1$ score without taking class imbalance into account. It treats all classes equally, regardless of their support.\n",
    "- **Weighted average**: This averages the precision, recall, and $F_1$ score, weighted by the support for each class. This means that the influence of each class's score on the overall average is proportional to the number of instances of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=model.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy\n",
    "\n",
    "The **cross-entropy loss** (also called *log loss*, which is the name used in scikit-learn) for a multi-class classification model is calculated as follows:\n",
    "$$\n",
    "\\text{Cross-Entropy} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log(p_{ik})\n",
    "$$\n",
    "where:\n",
    "- $n$ is the total number of observations,\n",
    "- $K$ is the number of classes,\n",
    "- $y_{ik}$ is a binary indicator: $1$ if class label $k$ is the correct classification for observation $i$, and $0$ otherwise,\n",
    "- $p_{ik}$ is the predicted probability that observation $i$ belongs to class $k$.\n",
    "\n",
    "Remember from the course that $\\argmin_{\\theta \\in \\Theta} \\mathbb{E}[-\\log p(Y; \\theta)] = \\argmin_{\\theta \\in \\Theta} \\text{KL}(p_\\theta, p)$.\n",
    "Since the distribution that minimizes the KL divergence is the true distribution, the expectation of the cross-entropy will be minimized when the model always predicts the correct vector of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) Predict probabilities using the `predict_proba` method of the logistic regression model. Then calculate the cross-entropy using the `log_loss` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)\n",
    "log_loss(y_test, y_pred_proba, labels=model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Based on `y_test_binarized`, compute the cross-entropy manually and check that it corresponds to the previous log loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "y_test_binarized = lb.transform(y_test)\n",
    "y_test_binarized.shape, y_test_binarized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "-(y_test_binarized * np.log(y_pred_proba)).sum(axis=1).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10) We will now experiment with various models for classification. For each one of the following models, we design a grid of hyperparameters based on the corresponding scikit-learn documentation:**\n",
    "- **[KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)**\n",
    "- **[Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)**\n",
    "- **[Linear Discriminant Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)**\n",
    "- **[Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html)**\n",
    "- **[Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** **(see also [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html))**\n",
    "- **[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**\n",
    "- **[Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)**\n",
    "\n",
    "**Make sure you understand all parameters; we will discuss random forests and gradient boosting later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-nearest neighbors\n",
    "param_grid_knn = {\n",
    "    'clf__n_neighbors': [3, 5, 10, 20, 50, 100], # K\n",
    "    'clf__weights': ['uniform', 'distance'], # Whether to weigh the neighbors equally or by their distance, as in locally weighted regression\n",
    "    'clf__p': [1, 2], # Norm 1 or norm 2, i.e., Manhattan or Euclidean distance\n",
    "}\n",
    "\n",
    "# Parametric naive Bayes assuming Gaussian distribution of the features.\n",
    "# This is a very simple model, so we don't need to tune any hyperparameters.\n",
    "param_grid_nb = {}\n",
    "\n",
    "# Linear discriminant analysis (LDA)\n",
    "param_grid_lda = {}\n",
    "\n",
    "# Quadratic discriminant analysis (QDA)\n",
    "# The parameter here is a regularization parameter that can be used to reduce overfitting, given that there are many parameters to learn.\n",
    "param_grid_qda = {\n",
    "    'clf__reg_param': [0, 0.1, 0.5, 1],\n",
    "} \n",
    "\n",
    "# In scikit-learn, logistic regression regularizes by default, so we need to specify the penalty and the regularization strength.\n",
    "param_grid_lr = {\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'clf__fit_intercept': [True, False],\n",
    "}\n",
    "\n",
    "# We additionally experiment with a logistic regression model with feature selection\n",
    "param_grid_lr_skb = {\n",
    "    'skb__k': [3, 5, 7, 9],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__fit_intercept': [True, False],\n",
    "}\n",
    "\n",
    "# Random forest\n",
    "param_grid_rf = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'n_estimators': [100, 200, 300], # Number of trees in the forest\n",
    "    'max_depth': [None, 2, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Gradient boosting\n",
    "param_grid_gb = {\n",
    "    'loss': ['log_loss'],\n",
    "    'learning_rate': [0.02, 0.1, 0.5],\n",
    "    'n_estimators': [100, 200, 300], # Number of boosting stages\n",
    "    'criterion': ['friedman_mse', 'squared_error'], # Function to measure the quality of a split\n",
    "    'max_depth': [None, 2, 5, 10],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11) For each one of these models, select the hyperparameters that give the lowest cross-entropy using the [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) class. We start by normalizing the data. Compute the accuracy and cross-entropy on the test dataset for the best hyperparameters (and store them in array).**\n",
    "\n",
    "**Print the best hyperparameters corresponding to each model and plot a confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "nb = GaussianNB()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "preprocessor = StandardScaler()\n",
    "knn = Pipeline([('pre', preprocessor), ('clf', knn)])\n",
    "nb = Pipeline([('pre', preprocessor), ('clf', nb)])\n",
    "lda = Pipeline([('pre', preprocessor), ('clf', lda)])\n",
    "qda = Pipeline([('pre', preprocessor), ('clf', qda)])\n",
    "lr = Pipeline([('pre', preprocessor), ('clf', lr)])\n",
    "lr_skb = Pipeline([('pre', preprocessor), ('skb', SelectKBest()), ('clf', LogisticRegression(max_iter=1000))])\n",
    "\n",
    "default_grid_params = dict(n_iter=10, cv=5, n_jobs=4, random_state=0)\n",
    "\n",
    "grids = {\n",
    "    'KNN': RandomizedSearchCV(knn, param_grid_knn, scoring='neg_log_loss', **default_grid_params),\n",
    "    'Naive Bayes': RandomizedSearchCV(nb, param_grid_nb, scoring='neg_log_loss', **default_grid_params),\n",
    "    'Linear Discriminant Analysis': RandomizedSearchCV(lda, param_grid_lda, scoring='neg_log_loss', **default_grid_params),\n",
    "    'Quadratic Discriminant Analysis': RandomizedSearchCV(lda, param_grid_lda, scoring='neg_log_loss', **default_grid_params),\n",
    "    'Logistic Regression': RandomizedSearchCV(lr, param_grid_lr, scoring='neg_log_loss', **default_grid_params),\n",
    "    'Logistic Regression with feature selection': RandomizedSearchCV(lr_skb, param_grid_lr_skb, scoring='neg_log_loss', **default_grid_params),\n",
    "    'Random Forest': RandomizedSearchCV(rf, param_grid_rf, scoring='neg_log_loss', **default_grid_params),\n",
    "    'Gradient Boosting': RandomizedSearchCV(gb, param_grid_gb, scoring='neg_log_loss', **default_grid_params),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for model_name, model in grids.items():\n",
    "    print('Running', model_name)\n",
    "    # Note that by default the argument `refit` of `RandomizedSearchCV` is set to True, so that the best estimator \n",
    "    # is refit on the whole training set.\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # We measure the test accuracy and log score.\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {test_accuracy:.3f}')\n",
    "\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    test_log_loss = log_loss(y_test, y_pred_proba, labels=model.classes_)\n",
    "    print(f'Log loss: {test_log_loss:.3f}')\n",
    "\n",
    "    results.append([test_accuracy, test_log_loss])\n",
    "    \n",
    "    print(f'Best hyperparameters: {model.best_params_}')\n",
    "\n",
    "    # We plot the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12) Create a pandas dataframe where each row corresponds to a model. The columns should correspond to the accuracy and log loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns=['Test accuracy', 'Test cross-entropy loss'], index=grids.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Exercise"
    ]
   },
   "source": [
    "We observe that Random Forest, Gradient Boosting, and KNN obtain the best accuracy.\n",
    "Random Forest and Gradient Boosting obtain the best cross-entropy value.\n",
    "Note that, on this dataset, KNN has a good accuracy, but its probabilistic predictions are quite poor, as indicated by the log loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
