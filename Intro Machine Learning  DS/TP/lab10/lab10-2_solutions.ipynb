{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bcef46",
   "metadata": {},
   "source": [
    "# Machine Learning 2024-2025 - UMONS\n",
    "\n",
    "# Gaussian Discriminant Analysis and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452411ce",
   "metadata": {
    "id": "452411ce",
    "papermill": {
     "duration": 2.180804,
     "end_time": "2022-08-01T09:09:22.172535",
     "exception": false,
     "start_time": "2022-08-01T09:09:19.991731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c346c",
   "metadata": {},
   "source": [
    "## 1. Gaussian Discriminant Analysis (GDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b27953",
   "metadata": {},
   "source": [
    "Nous considérons le jeu de données *IRIS*, qui est un dataset très classique (https://fr.wikipedia.org/wiki/Iris_de_Fisher). Celui-ci est inclus dans `scikit-learn`. Il contient 150 données sur 3 types d'iris (*Iris setosa*, *Iris virginica* et *Iris versicolor*). Les données ont 4 features : la longueur et la largeur des sépales, et la longueur et la largeur des pétales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_iris()\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.target, train_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb06d3-1528-4aeb-aa2b-b6888fe1d97c",
   "metadata": {},
   "source": [
    "On affiche les données selon les deux premières features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09326d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(dataset.data[:, 0], dataset.data[:, 1], c=dataset.target)\n",
    "ax.set(xlabel=dataset.feature_names[0], ylabel=dataset.feature_names[1])\n",
    "_ = ax.legend(\n",
    "    scatter.legend_elements()[0], dataset.target_names, loc=\"lower right\", title=\"Classes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c52c86-81b8-4419-8753-73049d00a803",
   "metadata": {},
   "source": [
    "On souhaite prédire le type d'iris en utilisant une analyse discriminante gaussienne. Pour cela, calculez les trois paramètres ($\\pi$, $\\mu$ et $\\Sigma$) décrivant la gaussienne de chaque classe d'iris. Vous pouvez utiliser `np.mean()` et `np.cov()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1abf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    n = y_train.shape[0] # Number of training examples.\n",
    "\n",
    "    x_train = x_train.reshape(n, -1)\n",
    "    p = x_train.shape[1] # Number of input features. In our case, 4.\n",
    "    class_label = len(np.unique(y_train.reshape(-1))) # Number of classes. In our case, 3.\n",
    "    \n",
    "    mu = np.zeros((class_label, p))\n",
    "    sigma = np.zeros((class_label, p, p))\n",
    "    pi = np.zeros(class_label)\n",
    "\n",
    "    for label in range(class_label):\n",
    "        indices = (y_train == label)\n",
    "        \n",
    "        pi[label] = float(np.sum(indices)) / n\n",
    "        mu[label] = np.mean(x_train[indices, :], axis=0)\n",
    "        sigma[label] = np.cov(x_train[indices, :], rowvar=0)\n",
    "    \n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589289f-7001-4c37-aae9-cfdd5d85e08b",
   "metadata": {},
   "source": [
    "À partir des paramètres calculés avec la fonction `fit`, implémentez une fonction `predict` qui calcule la classe la plus probable pour chacune des données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def predict(x_tests, pi, mu, sigma):\n",
    "    # flatten the test data\n",
    "    x_tests = x_tests.reshape(x_tests.shape[0], -1)\n",
    "    class_label = mu.shape[0] # Number of classes. In our case, k = 3.\n",
    "    scores = np.zeros((x_tests.shape[0], class_label)) \n",
    "    for label in range(class_label):\n",
    "        # normal_distribution_prob.logpdf will give us the log value of the distribution\n",
    "        normal_distribution_prob = multivariate_normal(mean=mu[label], cov=sigma[label])\n",
    "        # x_test can have multiple test data, we calculate the probability of each of the test data\n",
    "        for i, x_test in enumerate(x_tests):\n",
    "            scores[i, label] = np.log(pi[label]) + normal_distribution_prob.logpdf(x_test)\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95892e4-4158-40e9-87db-09c30f0ed7f7",
   "metadata": {},
   "source": [
    "Testez votre fonction `predict` sur les données de test. Pour évaluer la qualité de vos prédictions, calculez le score $F_1$ de vos prédictions. Vous pouvez utiliser la méthode `f1_score` de `scikit-learn` (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pi, mu, sigma = fit(x_train, y_train)\n",
    "y_predict = predict(x_test, pi, mu, sigma)\n",
    "score = f1_score(y_test, y_predict, average=\"weighted\")\n",
    "print(\"f1 score of our model: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac974f-4dce-40b9-b966-6d0b351da2e6",
   "metadata": {},
   "source": [
    "Comparez vos prédictions avec la méthode `LinearDiscriminantAnalysis` de `scikit-learn` (https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html).\n",
    "Il est possible que vos prédictions soient légèrement différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9110b6-46b4-47d3-9337-446dee436503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(x_train, y_train)\n",
    "y_predict_sk = lda.predict(x_test)\n",
    "print(\"f1 score of scikit-learn model is: \", f1_score(y_test, y_predict_sk, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1ac0a",
   "metadata": {},
   "source": [
    "## 2. Spam filters avec naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165dc96a",
   "metadata": {},
   "source": [
    "En utilisant `pandas`, on charge les données d'un dataset contenant des emails classifiés en spam ou non-spam (la valeur $1$ indique les spams, et la valeur $0$ les non-spams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5585a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6da5585a",
    "outputId": "5fe12449-e823-453a-8cd3-d8d551af9701",
    "papermill": {
     "duration": 0.458471,
     "end_time": "2022-08-01T09:09:22.642817",
     "exception": false,
     "start_time": "2022-08-01T09:09:22.184346",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/spam_email_raw_text_for_NLP.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d82db7",
   "metadata": {},
   "source": [
    "La colonne \"FILE_NAME\" n'est pas utile ; on la supprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7bed1",
   "metadata": {
    "id": "9ac7bed1",
    "papermill": {
     "duration": 0.029005,
     "end_time": "2022-08-01T09:09:22.684496",
     "exception": false,
     "start_time": "2022-08-01T09:09:22.655491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop('FILE_NAME',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd68699",
   "metadata": {},
   "source": [
    "On compte le nombre de spams et de non-spams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54014101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54014101",
    "outputId": "6290542a-8acf-4b46-c6e1-b79f581b0bfb",
    "papermill": {
     "duration": 0.028258,
     "end_time": "2022-08-01T09:09:22.725318",
     "exception": false,
     "start_time": "2022-08-01T09:09:22.697060",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82034812",
   "metadata": {},
   "source": [
    "Les \"stopwords\" sont les petits mots qui apparaissent dans la plupart des textes et sont peu pertinents pour la classification en spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b7577",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "742b7577",
    "outputId": "27872d10-273e-4c01-ddb9-d8baa30a071a",
    "papermill": {
     "duration": 0.028553,
     "end_time": "2022-08-01T09:09:22.806736",
     "exception": false,
     "start_time": "2022-08-01T09:09:22.778183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f43853",
   "metadata": {},
   "source": [
    "On crée le corpus. On le simplifie afin de retirer les caractères non-alphanumériques, les majuscules, et les caractères blancs. Le \"lemmatizer\" sert à identifier les mots similaires, par exemple \"chiens\" et \"chien\". Ce calcul peut prendre un peu de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe64c97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afe64c97",
    "outputId": "ae937941-bf60-456f-f0e9-54269b11830a",
    "papermill": {
     "duration": 353.681303,
     "end_time": "2022-08-01T09:15:17.567767",
     "exception": false,
     "start_time": "2022-08-01T09:09:23.886464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "corpus=[]\n",
    "for i in tqdm(range(len(df))):\n",
    "    # removing all non-alphanumeric characters\n",
    "    message = re.sub('[^a-zA-Z0-9]',' ',df['MESSAGE'][i]) \n",
    "    # converting the message to lowercase\n",
    "    message = message.lower() \n",
    "    # spliting the sentence into words for lemmatization                 \n",
    "    message = message.split()      \n",
    "    # removing stopwords and lemmatizing            \n",
    "    message = [lemmatizer.lemmatize(word) for word in message\n",
    "             if word not in set(stopwords.words('english'))] \n",
    "    # Converting the words back into sentences\n",
    "    message = ' '.join(message)    \n",
    "    # Adding the preprocessed message to the corpus list            \n",
    "    corpus.append(message)                 \n",
    "\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43668036",
   "metadata": {
    "id": "43668036",
    "papermill": {
     "duration": 0.015681,
     "end_time": "2022-08-01T09:15:26.794690",
     "exception": false,
     "start_time": "2022-08-01T09:15:26.779009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Pour chaque email, on crée un vecteur booléen qui donne la présence ou l'absence de mot (ou séquences de mots) dans le message. On utilise la méthode `CountVectorizer` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Pour limiter la taille des vecteurs, on se limite aux $2500$ mots les plus présents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94f89b",
   "metadata": {
    "id": "fa94f89b",
    "papermill": {
     "duration": 9.496483,
     "end_time": "2022-08-01T09:15:36.307174",
     "exception": false,
     "start_time": "2022-08-01T09:15:26.810691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 2500, binary = True)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df['CATEGORY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1ea5e-d104-44db-8911-ba99d4c72c0c",
   "metadata": {},
   "source": [
    "On sépare à nouveau les données en gardant $80 \\%$ de données d'entraînement et $20 \\%$ de données de test.\n",
    "\n",
    "Grâce à la variable `cv`, on peut facilement transformer un email en vecteur booléen, et on peut voir à quels mots correspondent les $2500$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b6698",
   "metadata": {
    "id": "e90b6698",
    "papermill": {
     "duration": 0.07569,
     "end_time": "2022-08-01T09:15:36.399509",
     "exception": false,
     "start_time": "2022-08-01T09:15:36.323819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1, stratify=y)\n",
    "print(cv.get_feature_names_out()[1100:1120]) # On affiche les mots correspondant à 20 features.\n",
    "message = [\"You won 10000 dollars, please provide your account details so that we can transfer the money\"]\n",
    "print(cv.transform(message))\n",
    "print(cv.get_feature_names_out()[1783], cv.get_feature_names_out()[2294])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be59ec-8513-4be0-9fbf-28e7d104d797",
   "metadata": {},
   "source": [
    "### 2.1 Naive Bayes à la main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55430b8-4112-463f-a1a0-b5afc698c779",
   "metadata": {},
   "source": [
    "Implémentez la méthode *naive Bayes* vue au cours pour prédire si un message donné est un spam ou non. Testez vos méthodes sur les données de test. Analysez vos résultats avec la méthode `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72f2ec-da5d-47e9-be59-75673adb3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    n = y_train.shape[0] # Number of training examples.\n",
    "    p = x_train.shape[1] # Number of input features. In our case, 2500.\n",
    "    x_train = x_train.reshape(n, -1)\n",
    "\n",
    "    # Laplace Smoothing: to ensure that all words are present in the vocabulary\n",
    "    # for each class, we add two dummy examples with each class label.\n",
    "    # Otherwise, if a word is not present in the training set, it will be assigned a probability of 0 and the likelihood will be 0.\n",
    "    x_train = np.append(x_train, [np.ones(p),np.ones(p)], axis=0)\n",
    "    y_train = np.append(y_train, [0,1])\n",
    "    n += 2\n",
    "    \n",
    "    pi = np.zeros(2)\n",
    "    phi_y = np.zeros((2, p))\n",
    "\n",
    "    for label in range(2):\n",
    "        indices = (y_train == label)     \n",
    "        pi[label] = float(np.sum(indices)) / n\n",
    "        phi_y[label] = np.mean(x_train[indices, :], axis=0)\n",
    "\n",
    "    return pi, phi_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0fd922-4309-40ae-bf5f-1d20f6ea2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_tests, pi, phi_y):\n",
    "    p = x_tests.shape[1] # Number of input features. In our case, 2500.\n",
    "    number_tests = x_tests.shape[0]\n",
    "    # flatten the test data\n",
    "    x_tests = x_tests.reshape(number_tests, -1)\n",
    "    scores = np.zeros((number_tests, 2)) \n",
    "    for i in range(number_tests):\n",
    "        for label in range(2):\n",
    "            scores[i, label] = np.log(pi[label])            \n",
    "            for j in range(p):\n",
    "                if x_tests[i,j]:\n",
    "                    scores[i, label] += np.log(phi_y[label][j])\n",
    "                else:\n",
    "                    scores[i, label] += np.log(1 - phi_y[label][j])\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d29930",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, phi_y = fit(x_train, y_train)\n",
    "train_pred = predict(x_train, pi, phi_y)\n",
    "test_pred = predict(x_test, pi, phi_y)\n",
    "print(classification_report(train_pred, y_train))\n",
    "print(classification_report(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting...')\n",
    "message = [\"You won 10000 dollars, please provide your account details so that we can transfer the money\"]\n",
    "message_vector = cv.transform(message)\n",
    "category = predict(message_vector, pi, phi_y)\n",
    "print(\"The message is\", \"spam\" if category == 1 else \"not spam\")\n",
    "\n",
    "print('Predicting...')\n",
    "message = [\"hey Laura, the meeting is postponed to Monday\"]\n",
    "message_vector = cv.transform(message)\n",
    "category = predict(message_vector, pi, phi_y)\n",
    "print(\"The message is\", \"spam\" if category == 1 else \"not spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596de3a",
   "metadata": {},
   "source": [
    "Les résultats ne sont pas très bons. En fait, le [*Laplace smoothing*](https://en.wikipedia.org/wiki/Additive_smoothing) peut être amélioré en pratique : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_smooth(x_train, y_train, alpha = 1):\n",
    "    n = y_train.shape[0] # Number of training examples.\n",
    "    p = x_train.shape[1] # Number of input features. In our case, 2500.\n",
    "    x_train = x_train.reshape(n, -1)\n",
    "\n",
    "    pi = np.zeros(2)\n",
    "    phi_y = np.zeros((2, p))\n",
    "\n",
    "    for label in range(2):\n",
    "        indices = (y_train == label)\n",
    "        sum_label = np.sum(indices)\n",
    "        pi[label] = float(sum_label) / n\n",
    "        for j in range(p):\n",
    "            phi_y[label][j] = (alpha + np.sum(x_train[indices,j])) / (sum_label + n * alpha)\n",
    "\n",
    "    return pi, phi_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912af4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phi, phi_y = fit_smooth(x_train, y_train)\n",
    "train_pred = predict(x_train, phi, phi_y)\n",
    "test_pred = predict(x_test, phi, phi_y)\n",
    "print(classification_report(train_pred, y_train))\n",
    "print(classification_report(test_pred, y_test))\n",
    "\n",
    "# Words that contribute the most to the spam score\n",
    "indices = np.argsort(phi_y[1])[::-1]\n",
    "print(\"Top 10 words that contribute the highest score to spams:\")\n",
    "for i in range(10):\n",
    "    print(cv.get_feature_names_out()[indices[i]], phi_y[1][indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a06272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting...')\n",
    "message = [\"You won 10000 dollars, please provide your account details so that we can transfer the money\"]\n",
    "message_vector = cv.transform(message)\n",
    "category = predict(message_vector, phi, phi_y)\n",
    "print(\"The message is\", \"spam\" if category == 1 else \"not spam\")\n",
    "\n",
    "print('Predicting...')\n",
    "message = [\"hey Laura, the meeting is postponed to Monday\"]\n",
    "message_vector = cv.transform(message)\n",
    "category = predict(message_vector, phi, phi_y)\n",
    "print(\"The message is\", \"spam\" if category == 1 else \"not spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d32a8d",
   "metadata": {
    "id": "09d32a8d",
    "papermill": {
     "duration": 0.016548,
     "end_time": "2022-08-01T09:15:51.102256",
     "exception": false,
     "start_time": "2022-08-01T09:15:51.085708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Naive Bayes avec Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa04b22-4480-4b62-9d40-193e3afd4b7d",
   "metadata": {},
   "source": [
    "Utilisez la méthode `MultinomialNB` de `scikit-learn` (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) pour effectuer la même tâche qu'en Section 2.1.\n",
    "\n",
    "Appliquez deux fois vos méthodes avec différentes valeurs pour le paramètre `alpha` correspondant au *Laplace smoothing*, et comparez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZCju8emD4o4T",
   "metadata": {
    "id": "ZCju8emD4o4T"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-zMcQYjR38FQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zMcQYjR38FQ",
    "outputId": "ebe409ec-f18c-44c0-bf14-486275707ad2"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bOdTDaQf4DzB",
   "metadata": {
    "id": "bOdTDaQf4DzB"
   },
   "outputs": [],
   "source": [
    "train_pred = model.predict(x_train)\n",
    "test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SjnvBa_04ETM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjnvBa_04ETM",
    "outputId": "17967dfa-644e-4e95-9fb9-f7aa6f583203"
   },
   "outputs": [],
   "source": [
    "print(classification_report(train_pred, y_train))\n",
    "print(classification_report(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8794a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef8794a9",
    "outputId": "bbe328ec-60f6-4532-e9ef-3550feda8cb2",
    "papermill": {
     "duration": 0.028498,
     "end_time": "2022-08-01T09:15:57.399679",
     "exception": false,
     "start_time": "2022-08-01T09:15:57.371181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Predicting...')\n",
    "message = [\"You won 10000 dollars, please provide your account details so that we can transfer the money\"]\n",
    "message_vector = cv.transform(message)\n",
    "category = model.predict(message_vector)\n",
    "print(\"The message is\", \"spam\" if category == 1 else \"not spam\")\n",
    "\n",
    "print('Predicting...')\n",
    "message = [\"hey Laura, the meeting is postponed to Monday\"]\n",
    "message_vector = cv.transform(message)\n",
    "category = model.predict(message_vector)\n",
    "print(\"The message is\", \"spam\" if category == 1 else \"not spam\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 409.166196,
   "end_time": "2022-08-01T09:15:59.008964",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-01T09:09:09.842768",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
