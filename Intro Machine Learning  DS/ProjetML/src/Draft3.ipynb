{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4DUj3n8UG8K"
      },
      "source": [
        "# Projet de Machine Learning  UMONS 2024-2025\n",
        "\n",
        "### Thème : Prédiction du score de Macron aux 2nd Tour des éléctions 2022\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74V_oeHHUG8N"
      },
      "source": [
        "# Configuration et Installation des Dépendances\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEWYuyMOzQNP",
        "outputId": "bcb12dcf-ad95-4139-978b-cbeca95035a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from sys import modules as sys_modules\n",
        "from os.path import join\n",
        "# chemin vers le dossier contenant toute les données à utiliser\n",
        "if 'google.colab' in sys_modules :\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  data_path = \"/content/drive/MyDrive/Colab_Notebooks/ProjetML/src/datasets\"\n",
        "else:\n",
        "  data_path = \"datasets\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bJStL1hzQNQ",
        "outputId": "1ef9c3f1-cb7b-444b-f353-645fb10d7c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in sys_modules:\n",
        "      bib = [\"openpyxl\",\n",
        "             \"xlrd\",\n",
        "             \"optuna\",\n",
        "             \"lightgbm\",\n",
        "             \"xgboost\"\n",
        "            ]\n",
        "      for b in bib:\n",
        "        %pip install {b}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMocDeJneTg"
      },
      "source": [
        "## a. Import des Bibliothèques/dépendances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "cellView": "form",
        "id": "voOMOwwxneTg"
      },
      "outputs": [],
      "source": [
        "# @title manipulation des vecteurs\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "cellView": "form",
        "id": "nafKS3iMneTh"
      },
      "outputs": [],
      "source": [
        "# @title création des graphiques\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as mtick\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "cellView": "form",
        "id": "SZmFe7zUneTi"
      },
      "outputs": [],
      "source": [
        "# @title prétraitement des données en masse\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler , FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer , make_column_transformer\n",
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "1u_RbAg4neTi"
      },
      "outputs": [],
      "source": [
        "# @title selection des features\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from functools import lru_cache\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LassoCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ZBWGUixqneTi"
      },
      "outputs": [],
      "source": [
        "# @title sélection du meilleur modèle\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.exceptions import TrialPruned\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score , RepeatedKFold\n",
        "from typing import Dict, Any, Tuple , List\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "cellView": "form",
        "id": "NGC4EQWKneTj"
      },
      "outputs": [],
      "source": [
        "# @title fonction de score et evaluation\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_squared_error, r2_score ,mean_absolute_error\n",
        "from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "_ZFZ3fStneTj"
      },
      "outputs": [],
      "source": [
        "# @title initialisation des modèles\n",
        "from sklearn.linear_model import ElasticNet , Lasso\n",
        "from xgboost import XGBRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "cellView": "form",
        "id": "mr3fyJ4LneTk"
      },
      "outputs": [],
      "source": [
        "# @title options sytèmes\n",
        "from sys import modules as sys_modules\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from joblib import Parallel , delayed\n",
        "import json\n",
        "import warnings\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellView": "form",
        "id": "l86laxxtXd-J",
        "outputId": "dcd4e186-9dbc-4dbc-d534-96322e33c6ea"
      },
      "source": [
        "## b. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR0LDcONneTl",
        "outputId": "4c1c9706-d16d-409f-9b74-2229f8f7c8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chargement des bibliothèques terminé\n"
          ]
        }
      ],
      "source": [
        "#Pour ignorer les warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# configuration des graphiques\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_palette('Set2')\n",
        "\n",
        "# Pour une meilleure lisibilité dans le notebook\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.float_format', '{:.3f}'.format)\n",
        "\n",
        "print(\"chargement des bibliothèques terminé\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VJxyPKWSya9"
      },
      "source": [
        "## c. Constantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "oFKQKuWfSya9"
      },
      "outputs": [],
      "source": [
        "# @title Paramètres globaux\n",
        "CORRTHRESHOLD = 60 # @param {\"type\":\"integer\"}\n",
        "RANDOM_STATE = 42 # @param {\"type\":\"integer\"}\n",
        "IDs= [] # @param {type:\"raw\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7CvwbxZSya9"
      },
      "source": [
        "## d. Fonctions utilitaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "cellView": "form",
        "id": "1eqgTCBloy3C"
      },
      "outputs": [],
      "source": [
        "# @title Utilitaires\n",
        "\n",
        "def sep(lg=90):\n",
        "    \"\"\"Affiche une ligne de séparation\"\"\"\n",
        "    print(\"\\n\" + \"-\"*lg + \"\\n\")\n",
        "\n",
        "def sub(l1,l2):\n",
        "  \"\"\"Retourne la liste des éléments de l1 qui ne sont pas dans l2\"\"\"\n",
        "  return [x for x in l1 if x not in l2]\n",
        "\n",
        "def get_columns_above_missing_threshold(df, threshold:int=CORRTHRESHOLD):\n",
        "    \"\"\"\n",
        "    Identifie les colonnes ayant un pourcentage de valeurs manquantes supérieur au seuil spécifié.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Le DataFrame à analyser\n",
        "        threshold (float): Le seuil en pourcentage (entre 0 et 100) au-delà duquel une colonne est considérée\n",
        "                          comme ayant trop de valeurs manquantes. Par défaut : 50\n",
        "\n",
        "    Returns:\n",
        "        list: Liste des noms de colonnes dont le pourcentage de valeurs manquantes dépasse le seuil,\n",
        "              triée par pourcentage décroissant\n",
        "    \"\"\"\n",
        "    # Vérification que le seuil est valide\n",
        "    if not 0 <= threshold <= 100:\n",
        "        raise ValueError(\"Le seuil doit être compris entre 0 et 100\")\n",
        "\n",
        "    # Calcul du pourcentage de valeurs manquantes par colonne\n",
        "    missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "    # Sélection des colonnes dépassant le seuil\n",
        "    columns_above_threshold = missing_percentages[missing_percentages > threshold]\n",
        "\n",
        "    # Tri par pourcentage décroissant\n",
        "    columns_above_threshold = columns_above_threshold.sort_values(ascending=False)\n",
        "\n",
        "    # Création d'un DataFrame avec les colonnes et leurs pourcentages\n",
        "    \"\"\"result_df = pd.DataFrame({\n",
        "        'Colonne': columns_above_threshold.index,\n",
        "        'Pourcentage de valeurs manquantes': columns_above_threshold.values\n",
        "    })\"\"\"\n",
        "\n",
        "    return  columns_above_threshold.index.to_list() , pd.DataFrame(columns_above_threshold)\n",
        "\n",
        "def visualise(df):\n",
        "  print(f\"forme : {df.shape}\")\n",
        "\n",
        "def all_columns(df, res=True):\n",
        "  all_col = df.columns.tolist()\n",
        "  print(f\"les colonnes sont : \\n{df.columns}\")\n",
        "  print(f\" il y'a {len(all_col)} colonnes dans le dataframe\")\n",
        "  sep()\n",
        "  if res:\n",
        "    return all_col\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def get_analyse(data , id , col_to_drop , res =True):\n",
        "\n",
        "    df= data.copy()\n",
        "    all_col = df.columns.tolist()\n",
        "    col_to_keep = sub(all_col,col_to_drop)\n",
        "    to_holes,_ = get_columns_above_missing_threshold(df[col_to_keep])\n",
        "    col_to_keep2 = sub(col_to_keep,to_holes)\n",
        "    IDs.append(id)\n",
        "    if res:\n",
        "      return col_to_keep2 , to_holes\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "def write_markdown_conclusion(id_colonne, colonnes_manquantes, colonnes_supprimer, colonnes_conserver):\n",
        "    \"\"\"\n",
        "    Génère une section Markdown pour la conclusion de l'analyse d'un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        id_colonne (str): Nom de la colonne d'identification.\n",
        "        colonnes_manquantes (list): Liste des colonnes avec un fort taux de valeurs manquantes.\n",
        "        colonnes_supprimer (list): Liste des colonnes à supprimer.\n",
        "        colonnes_conserver (list): Liste des colonnes à conserver.\n",
        "    \"\"\"\n",
        "\n",
        "    markdown_text = f\"\"\"\n",
        "#### Conclusion\n",
        "\n",
        "* _Identifiant_ : ``'{id_colonne}'``\n",
        "\n",
        "* _Colonnes avec plus de {CORRTHRESHOLD}% de valeurs manquantes_ : ``{colonnes_manquantes}``\n",
        "\n",
        "* _Colonnes à supprimer_ : ``{colonnes_supprimer}``\n",
        "\n",
        "* _Colonnes à conserver_ : ``{colonnes_conserver}``\n",
        "\n",
        "----\n",
        "\"\"\"\n",
        "    display(Markdown(markdown_text))\n",
        "\n",
        "def display_correlation_matrix(df , save = False,name=None , table=False , target=None, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Affiche la matrice de corrélation entre les colonnes numériques du DataFrame et la colonne cible.\n",
        "\n",
        "    Args:\n",
        "    ----\n",
        "    df : pd.DataFrame\n",
        "        Le DataFrame à analyser.\n",
        "    target : str\n",
        "        Le nom de la colonne cible.\n",
        "    threshold : float\n",
        "        Le seuil de corrélation au-delà duquel les colonnes sont considérées comme corrélées.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    t_in = (target in df.columns)\n",
        "\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    # Calculer la matrice de corrélation\n",
        "\n",
        "    if len(num_cols) == 0:\n",
        "        raise ValueError(\"Aucune colonne numérique trouvée dans le DataFrame.\")\n",
        "\n",
        "    if (target is not None) and t_in:\n",
        "        num_cols = [target] + [col for col in num_cols if col != target]\n",
        "\n",
        "    corr_matrix = df[num_cols].corr()\n",
        "\n",
        "    # Masquer la moitié inférieure\n",
        "    upper = corr_matrix.where(np.tril(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # filter les correlations dépassant le seuil\n",
        "    filtered_corr = upper[\n",
        "                          (upper.abs() > threshold) &\n",
        "                          (upper != 1.0)\n",
        "                          ].dropna(how='all',axis=0).dropna(axis=1, how='all').sort_values(by=target, ascending=False) if t_in else upper[ (upper.abs() > threshold) & (upper != 1.0)].dropna(how='all',axis=0).dropna(axis=1, how='all').sort_values(ascending=False)\n",
        "\n",
        "    if t_in:\n",
        "        corr_with_target=  corr_matrix[target].drop(target).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "        if table :\n",
        "            filt_title = f\"Tableau de corrélation entre les colonnes dépassnt  {threshold} : \\n\"\n",
        "            display(Markdown(filt_title))\n",
        "            display(filtered_corr)\n",
        "            display(Markdown(\"-\"*50))\n",
        "            with_ta_title = f\"Tableau de corrélation entre les colonnes et la colonne {target} : \\n\"\n",
        "            display(Markdown(with_ta_title))\n",
        "            display(corr_with_target)\n",
        "\n",
        "        # Afficher la matrice de corrélation\n",
        "        fig , (ax1 , ax2) = plt.subplots( 1 , 2 , figsize=(12, 8))\n",
        "        sns.heatmap( filtered_corr , ax= ax1 , annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "        ax1.set_title(f\"Matrice de corrélation (seuil : {threshold})\")\n",
        "        sns.heatmap( corr_with_target , ax= ax2 , annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "        ax2.set_title(f\"Matrice de corrélation avec {target}\")\n",
        "        plt.legend(loc='upper right', fontsize=10)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save:\n",
        "            if name is None:\n",
        "                plt.savefig(\"corr_matrix.png\")\n",
        "            else:\n",
        "                plt.savefig(f\"corr_matrix_{name}.png\")\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        if table :\n",
        "            display(Markdown(f\"La colonne cible '{target}' n'existe pas dans le DataFrame.\"))\n",
        "            display(Markdown(\"-\"*50))\n",
        "            filt_title = f\"Tableau de corrélation entre les colonnes dépassnt  {threshold} : \\n\"\n",
        "            display(Markdown(filt_title))\n",
        "            display(filtered_corr)\n",
        "\n",
        "        # Afficher la matrice de corrélation\n",
        "        plt.figsize=(12, 8)\n",
        "        sns.heatmap( filtered_corr  , annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "        plt.title(f\"Matrice de corrélation (seuil : {threshold})\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save:\n",
        "            if name is None:\n",
        "                plt.savefig(\"corr_matrix.png\")\n",
        "            else:\n",
        "                plt.savefig(f\"corr_matrix_{name}.png\")\n",
        "\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCiAJj37UG8P"
      },
      "source": [
        "# 1. EXPLORATION DES DONNEES\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YN4dXiS-mkI"
      },
      "source": [
        "\n",
        "Le but ici c'est d'essayer de comprendre  les données , c'est pouvoir repondre aux questions :\n",
        "* Quelles sont les données visiblement non-pertinentes ?\n",
        "* Detecter les outliers ?\n",
        "* Vérifier le taux de valeurs manquantes\n",
        "* regrouper les informations en indices synthétiques\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxY6oi4VUG8P"
      },
      "source": [
        "## 1.1 Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4-a9liSSya_",
        "outputId": "622b9bdc-69c9-41eb-dea9-68ad32267008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debut du chargement des données ... .. ... ..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rOptimisation des hyperparamètres:  30%|███       | 3/10 [1:58:28<4:36:27, 2369.58s/itération, Best RMSE=21.6]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"debut du chargement des données ... .. ... ..\")\n",
        "\n",
        "# Chargement des données de d'entrainement et de test\n",
        "result_train = pd.read_csv( os.path.join(data_path,\"results_train.csv\") , sep = ',',encoding='utf-8')\n",
        "result_test = pd.read_csv( os.path.join(data_path,\"results_test.csv\") , sep = ',',encoding='utf-8')\n",
        "\n",
        "res_train_df = result_train.copy()\n",
        "res_test_df = result_test.copy()\n",
        "\n",
        "#-------------------------\n",
        "# Données additionnelles\n",
        "#------------------------\n",
        "\n",
        "#  Niveau de vie\n",
        "niveau_vie = pd.read_excel(os.path.join(data_path, \"Niveau_de_vie_2013_a_la_commune.xlsx\"))\n",
        "niveau_vie_df = niveau_vie.copy()\n",
        "\n",
        "#  Communes de France\n",
        "communes_france = pd.read_csv(os.path.join(data_path, \"communes-france-2022.csv\"), sep=',', encoding='utf-8')\n",
        "communes_df = communes_france.copy()\n",
        "\n",
        "#  Données d'âge\n",
        "age_insee = pd.read_excel(os.path.join(data_path, \"age-insee-2020.xlsx\"))\n",
        "age_df = age_insee.copy()\n",
        "\n",
        "# Données diverses INSEE\n",
        "insee_divers = pd.read_excel(os.path.join(data_path, \"MDB-INSEE-V2.xls\"))\n",
        "insee_divers_df = insee_divers.copy()\n",
        "\n",
        "print(\"chargement des données terminé !! \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGWVcP42UG8Q"
      },
      "source": [
        "## 1.2 Pré-Analyse  et Pré-Selection(visuelle) des Features (colonnes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dpzBSJElb34y"
      },
      "outputs": [],
      "source": [
        "# @title colonne cible\n",
        "target = '% Voix/Ins' # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OdTiFiNneTn"
      },
      "source": [
        "### results_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "sauBteSqoi6T"
      },
      "outputs": [],
      "source": [
        "# @title visualisation\n",
        "visualise(res_train_df)\n",
        "\n",
        "res_train_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "mg0k3FwRneTo"
      },
      "outputs": [],
      "source": [
        "# @title affichage des colonnes\n",
        "all_col_train = all_columns(res_train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "k4cdKi25pmJw"
      },
      "outputs": [],
      "source": [
        "# présélection\n",
        "# colonnes clairement non informatives à supprimer\n",
        "col_to_drop_train = ['Unnamed: 27' ,'Unnamed: 26' , 'Unnamed: 28' ,\n",
        "                'Unnamed: 29' , 'Unnamed: 30','Unnamed: 31' ,\n",
        "                'Unnamed: 32' ,'Prénom','Sexe','Nom','N°Panneau','Libellé de la commune']\n",
        "\n",
        "id_train = 'CodeINSEE'\n",
        "\n",
        "col_to_keep1 , to_holes_train = get_analyse(res_train_df,id_train,col_to_drop_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5HXsbklr4-Do"
      },
      "outputs": [],
      "source": [
        "# @title conclusion\n",
        "write_markdown_conclusion(id_train, to_holes_train, col_to_drop_train, col_to_keep1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww_frSK5neTo"
      },
      "source": [
        "### niveau de vie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "WdpNVxKZ2WzD"
      },
      "outputs": [],
      "source": [
        "# @title visualisation\n",
        "visualise(niveau_vie_df)\n",
        "\n",
        "niveau_vie_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bHwUpeZQskkr"
      },
      "outputs": [],
      "source": [
        "all_col_niveau_vie = all_columns(niveau_vie_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "rvA-sahvjfSh"
      },
      "outputs": [],
      "source": [
        "# @title présélection\n",
        "# colonnes clairement non informatives à supprimer\n",
        "col_to_drop_niveau = []\n",
        "id_niveau = 'Code Commune'\n",
        "\n",
        "col_to_keep2 , to_holes_niveau = get_analyse(niveau_vie_df,id_niveau,col_to_drop_niveau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "rk3ZUZHip3yq"
      },
      "outputs": [],
      "source": [
        "# @title conclusion\n",
        "write_markdown_conclusion(id_niveau, to_holes_niveau, col_to_drop_niveau, col_to_keep2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zhRxcQNuUvM"
      },
      "source": [
        "### communes de frances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "gElHK_hC6vhG"
      },
      "outputs": [],
      "source": [
        "# @title visualisation\n",
        "visualise(communes_df)\n",
        "\n",
        "communes_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "64-M4DMxufK5"
      },
      "outputs": [],
      "source": [
        "# @title all colonnes\n",
        "all_col_commune = all_columns(communes_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66ZazhT7YDk"
      },
      "source": [
        "A première vu , on n'a pas besoin des\n",
        "* url vers les sites internet des communes  c'est à dire ``url_wikipedia`` , ``url_ville``\n",
        "* ``typecom`` et ``typecom_texte`` sont des colonnes constantes : on est toujours censé avoir à faire à des communes\n",
        "* pas besoin de tous les type de noms de la communes , un seul suffira `nom_standard` , mais il est aussi renseigné dans `Niveau de vie`\n",
        "* unamed ici represente un index donc inutile aussi\n",
        "* on peut regrouper ` 'altitude_moyenne'`, `'altitude_minimale'`,\n",
        "       `'altitude_maximale'` en  un ou deux indices d'altitude , idem pour les ``latitude...`` et ``longitute...``\n",
        "* supprimer les infos d'identification (x_code , x_nom) sur les départements et régions , leurs codes sont déjà fourni dans ``MDB-insee-divers`` ce dernier ayant plus d'échantillons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "G2eJw4BX7sCX"
      },
      "outputs": [],
      "source": [
        "# @title présélection\n",
        "# colonnes jugées non informatives à supprimer\n",
        "\n",
        "col_to_drop_commune = ['url_wikipedia','url_villedereve',\n",
        "                       'typecom','typecom_texte',\n",
        "                       'nom_standard','nom_a','nom_de',\n",
        "                       'nom_sans_pronom','gentile',\n",
        "                       'nom_sans_accent','nom_standard_majuscule',\n",
        "                       'superficie_hectare','Unnamed: 0',\n",
        "                       'academie_nom','codes postaux',\n",
        "                       'longitude_centre','latitude_centre',\n",
        "                       'reg_nom','dep_nom','canton_nom','epci_nom']\n",
        "id_commune = 'code_insee'\n",
        "\n",
        "col_to_keep3 , to_holes_commune = get_analyse(communes_df,id_commune,col_to_drop_commune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "PTLPZL5u-k3W"
      },
      "outputs": [],
      "source": [
        "# @title conclusion\n",
        "write_markdown_conclusion(id_commune, to_holes_commune, col_to_drop_commune, col_to_keep3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExVYdARYuZTa"
      },
      "source": [
        "### age-insee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wljv5s_WSybQ"
      },
      "source": [
        "#### première analyse visuelle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eTedhaBj_5mn"
      },
      "outputs": [],
      "source": [
        "# @title visualisation\n",
        "visualise(age_df)\n",
        "\n",
        "age_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "t7w804f5Biks"
      },
      "outputs": [],
      "source": [
        "# @title Présélection\n",
        "# visuellement\n",
        "col_to_drop_age = ['NOM']\n",
        "id_age = 'INSEE'\n",
        "all_col_age = age_df.columns.tolist()\n",
        "col_to_keep4 , to_holes_age = get_analyse(age_df,id_age,col_to_drop_age)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY9updnnSybR"
      },
      "outputs": [],
      "source": [
        "col_to_drop_age += to_holes_age\n",
        "print(col_to_drop_age)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OrmGPTjSybR"
      },
      "source": [
        "A defaut d'avoir toutes les tranches d'âge pour chaque sexe , on regroupe les nombres d'hommes et femmes de chaque tranche pour avoir moins de variables:\n",
        "* Sur le plan social  :\n",
        "    * `% Mineurs` (0 - 17),\n",
        "    * `% Adultes`(18-54) ,\n",
        "    * `% Seniors`(55-79) ,\n",
        "    * `% Tres_seniors`(80+)\n",
        "* sur le plan économique :\n",
        "    *`% Travailleurs` ceux qui ont l'age de potentiellement travailler\n",
        "    * `% Retraites`  ceux qu'on a jugés ne plus pouvoir travailler car ont plus de `64` ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0fed0mFSybR"
      },
      "source": [
        "#### Regroupement des âges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tclIXu5cSybS"
      },
      "outputs": [],
      "source": [
        "age_groups = age_df[age_df.columns[:5]]\n",
        "\n",
        "# population total\n",
        "col_genre = sub(sub(all_col_age,col_to_drop_age),age_df.columns[:5].tolist())\n",
        "col_hom = col_genre[10:]\n",
        "age_groups['Population'] = age_df[col_genre].sum(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EExcLRi4SybS"
      },
      "outputs": [],
      "source": [
        "#--------------------------------\n",
        "# Regroupement par cycle de vie\n",
        "#-------------------------------\n",
        "\n",
        "# les mineurs\n",
        "age_groups['% Mineurs'] = (age_df['F0-2'] + age_df['F3-5'] +age_df['F6-10']+age_df['F11-17'] + age_df['H0-2'] + age_df['H3-5'] + age_df['H0-2']+age_df['H6-10']+age_df['F11-17'] ) / age_groups['Population'] * 100\n",
        "\n",
        "# les adultes\n",
        "age_groups['% Adultes'] = (age_df['F18-24'] + age_df['F25-39'] + age_df['F40-54']+age_df['H18-24']+age_df['H25-39'] + age_df['H40-54']) / age_groups['Population'] * 100\n",
        "\n",
        "\n",
        "# les agés\n",
        "age_groups['% Seniors'] = (age_df['F55-64'] + age_df['F65-79'] + age_df['H55-64'] + age_df['H65-79']) / age_groups['Population'] * 100\n",
        "\n",
        "\n",
        "# les très agés\n",
        "age_groups['% Tres_Seniors'] = (age_df['F80+'] + age_df['H80+']) / age_groups['Population'] * 100\n",
        "\n",
        "# les retraités\n",
        "age_groups['% Retraites'] = (age_df['F65-79'] + age_df['H65-79'] + age_df['F80+'] + age_df['H80+']) / age_groups['Population'] * 100\n",
        "\n",
        "# travailleurs potentiels\n",
        "age_groups['% Travailleurs'] = (age_groups['% Adultes'] + age_groups['% Seniors'] + age_groups['% Tres_Seniors'] - age_groups['% Retraites']) / age_groups['Population'] * 100\n",
        "\n",
        "# ratio hommes/femmes\n",
        "age_groups['rH/F'] = ( age_df[col_hom].sum(axis=1)  / age_df[col_genre[:10]].sum(axis=1) )\n",
        "\n",
        "age_groups.to_csv(\"age_groups.csv\",index=False)\n",
        "age_groups.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTVbAj7fSybS"
      },
      "source": [
        "#### mise à jour et conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCVhAFdtSybS"
      },
      "outputs": [],
      "source": [
        "col_to_drop_age = []\n",
        "colt_to_keep4 = sub(all_col_age,col_to_drop_age)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruSXcXDEibgU"
      },
      "source": [
        "### MDB-INSEE-Divers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn8zV1bxSybT"
      },
      "source": [
        "#### première analyse visuelle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "vgg4eqWziuGG"
      },
      "outputs": [],
      "source": [
        "# @title visualisation\n",
        "visualise(insee_divers_df)\n",
        "\n",
        "insee_divers_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "tlJjQVfUi4Jt"
      },
      "outputs": [],
      "source": [
        "# @title affichage des colonnes\n",
        "all_col_insee_divers = all_columns(insee_divers_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJRXgKjVSybT"
      },
      "source": [
        "#### regroupement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ptsZtzUjjNmB"
      },
      "outputs": [],
      "source": [
        "# @title présélection\n",
        "id_mdb = 'CODGEO'\n",
        "# colonnes jugées non informatives ou trop difficile à manipuler\n",
        "col_to_drop_mdb = ['Nb Résidences Principales','Nb Log Vacants' ,\n",
        "                   'Nb Résidences Secondaires',\n",
        "                   'Score démographique' , 'Score Ménages' ,\n",
        "                   'Evolution Pop %','Score Fiscal',\n",
        "                   'Score Evasion Client','Score Synergie Médicale',\n",
        "                   'Population','Nb Logement Secondaires',\n",
        "                   'Capacité Camping', 'Capacité Hotel',\n",
        "                   'Dynamique Démographique BV', 'Capacité Fisc',\n",
        "                   'CP' , 'Urbanité Ruralité' , 'SEG Environnement Démographique Obsolète']\n",
        "# les colonnes qui on été regroupées\n",
        "HF = ['Nb Homme' , 'Nb Femme']\n",
        "\n",
        "mM = ['Nb Mineurs' , 'Nb Majeurs']\n",
        "\n",
        "Nb_entreprises = ['Nb Entreprises Secteur Commerce' ,\n",
        "                  'Nb Entreprises Secteur Construction',\n",
        "                  'Nb Entreprises Secteur Industrie' ,\n",
        "                  'Nb Entreprises Secteur Services']\n",
        "\n",
        "Nb_medecins = ['Nb Omnipraticiens BV' ,\n",
        "             'Nb Infirmiers Libéraux BV',\n",
        "             'Nb dentistes Libéraux BV',\n",
        "             'Nb pharmaciens Libéraux BV']\n",
        "\n",
        "Indice_creation_eco = ['Nb Création Enteprises' ,\n",
        "                    'Nb Création Industrielles',\n",
        "                    'Nb Création Construction',\n",
        "                    'Nb Création Commerces',\n",
        "                    'Nb création Services']\n",
        "\n",
        "Indice_fiscal = ['Moyenne Revenus Fiscaux Départementaux',\n",
        "                 'Moyenne Revenus Fiscaux Régionaux']\n",
        "\n",
        "Indice_social =['Nb Education, santé, action sociale',\n",
        "                'Nb Santé, action sociale',\n",
        "                'Nb institution de Education, santé, action sociale, administration']\n",
        "\n",
        "Indice_services = ['Nb Services personnels et domestiques',\n",
        "                   'Nb Industries des biens intermédiaires',\n",
        "                   'Nb de Commerce',\n",
        "                   'Nb de Services aux particuliers']\n",
        "\n",
        "Indice_salaire_global = ['Dep Moyenne Salaires Horaires',\n",
        "                         'Reg Moyenne Salaires Horaires',]\n",
        "\n",
        "indice_salaire_csp = ['Dep Moyenne Salaires Cadre Horaires',\n",
        "                      'Reg Moyenne Salaires Cadre Horaires',\n",
        "                      'Dep Moyenne Salaires Prof Intermédiaire Horaires',\n",
        "                      'Reg Moyenne Salaires Prof Intermédiaire Horaires',\n",
        "                      'Dep Moyenne Salaires Employé Horaires',\n",
        "                      'Reg Moyenne Salaires Employé Horaires',\n",
        "                      'Dep Moyenne Salaires Ouvrié Horaires',\n",
        "                      'Reg Moyenne Salaires Ouvrié Horaires']\n",
        "\n",
        "dict_indice_salaire = {\n",
        "    'indice_salaire_global': Indice_salaire_global,\n",
        "    **{'indice_salaire_'+str(i) : indice_salaire_csp[i:i+2] for i in range(0, len(indice_salaire_csp), 2)}\n",
        "}\n",
        "\n",
        "col_to_drop_mdb += HF + mM + Nb_entreprises + Nb_medecins + Indice_creation_eco + Indice_fiscal + Indice_salaire_global + indice_salaire_csp + Indice_services + Indice_social\n",
        "\n",
        "col_to_keep5 = sub(all_col_insee_divers,col_to_drop_mdb)\n",
        "\n",
        "# ...existing code...\n",
        "\n",
        "# Dictionnaire de correspondance pour les nouveaux noms\n",
        "nouveaux_noms = {\n",
        "    'indice_salaire_0': 'indice_salaire_cadre',\n",
        "    'indice_salaire_2': 'indice_salaire_prof',\n",
        "    'indice_salaire_4': 'indice_salaire_employé',\n",
        "    'indice_salaire_6': 'indice_salaire_ouvrié'\n",
        "}\n",
        "\n",
        "for old_key, new_key in nouveaux_noms.items():\n",
        "    if old_key in dict_indice_salaire:\n",
        "        dict_indice_salaire[new_key] = dict_indice_salaire.pop(old_key)\n",
        "\n",
        "# Vérification\n",
        "for k, v in dict_indice_salaire.items():\n",
        "    print(f\"{k} : {v}\")\n",
        "# ...existing code..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrUs9nYFSybU"
      },
      "outputs": [],
      "source": [
        "mdb_df = insee_divers_df[col_to_keep5].copy()\n",
        "\n",
        "# nombre d'entreprises\n",
        "mdb_df['Nb Entreprises'] = insee_divers_df[Nb_entreprises].sum(axis=1)\n",
        "\n",
        "# nombre de médecins\n",
        "mdb_df['Nb Medecins'] = insee_divers_df[Nb_medecins].sum(axis=1)\n",
        "mdb_df.head(3)\n",
        "\n",
        "# ratio du nombre d'hommes par rapport au femmes\n",
        "mdb_df['rH/F'] = insee_divers_df[HF[0]] / insee_divers_df[HF[1]]\n",
        "\n",
        "# ratio du nombre de mineurs par par rapport au majeurs\n",
        "mdb_df['rm/M'] = insee_divers_df[mM[0]] / insee_divers_df[mM[1]]\n",
        "\n",
        "# Indice salariax\n",
        "for k , v in dict_indice_salaire.items():\n",
        "    mdb_df[k] = insee_divers_df[v[0]]/insee_divers_df[v[1]]\n",
        "\n",
        "# indice social\n",
        "mdb_df['indice_social'] = insee_divers_df[Indice_social].sum(axis=1)\n",
        "\n",
        "# indice services\n",
        "mdb_df['indice_services'] = insee_divers_df[Indice_services].sum(axis=1)\n",
        "\n",
        "mdb_df.to_csv(\"MDB_groupe.csv\",index=False)\n",
        "mdb_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMLni9ad0AXr"
      },
      "source": [
        "* pas besoin du `Nb Résidences Principales` et `Nb Résidences Secondaires` , `Nb log Vacants`  car leur somme = `Nb Logement`\n",
        "* on peut regrouper les colonnes `Moyenne Revenus Fiscaux Départementaux` à `Moyenne Revenus Fiscaux Régionaux` en `indice fiscal` par  Indice_fiscal_dep $= \\frac{Moyenne Revenus Fiscaux Départementaux}{Moyenne Revenus Fiscaux Régionaux}$\n",
        "\n",
        "* regrouper  `Dep Moyenne Salaires Horaires` à `Reg Moyenne Salaires Horaires`  en indice salariaux -> utiliser la méthodes ACP apprament qui permet de les réduire à 2 indices synthétiques `CP1` et `CP2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVpqMyjgUG8R"
      },
      "source": [
        "## 1.3 Fusion des sources fournies et nettoyage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F66BiGGSybV"
      },
      "source": [
        "### resultat de la préanalyse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "D_w4XlvH0fbN"
      },
      "outputs": [],
      "source": [
        "# @title colonnes à supprimer , avec trop de valeurs manquantes et à conserver\n",
        "\n",
        "cols_to_drop = col_to_drop_train + col_to_drop_niveau + col_to_drop_commune + col_to_drop_age + col_to_drop_mdb\n",
        "print(f\"{len(cols_to_drop)} colonnes(s) à supprimer\")\n",
        "\n",
        "cols_to_keep = col_to_keep1 + col_to_keep2 + col_to_keep3 + col_to_keep4 + col_to_keep5\n",
        "print(f\"{len(cols_to_keep)} colonne(s) à conserver y compris les identifiants\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHi6J682SybV"
      },
      "source": [
        "### fusion et extration des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NcQg3GPtzijy"
      },
      "outputs": [],
      "source": [
        "def safe_feature_selection(X, selected_features):\n",
        "    \"\"\"\n",
        "    Retourne les colonnes de X correspondant aux selected_features valides.\n",
        "    \"\"\"\n",
        "    return X[[feat for feat in selected_features if feat in X.columns]]\n",
        "\n",
        "merge_list = [(niveau_vie_df , id_niveau),\n",
        "              (communes_df , id_commune),\n",
        "              (age_groups , id_age),\n",
        "              (mdb_df , id_mdb)]\n",
        "\n",
        "#fonction de fussion\n",
        "def prepare_datasets(train_data,\n",
        "                     test_data,\n",
        "                     merge_list ,\n",
        "                     base_id = id_train,\n",
        "                     cols_to_drop = cols_to_drop,\n",
        "                     verbose=False):\n",
        "    \"\"\"\n",
        "    Fonction pour préparer et fusionner les datasets pour la modélisation\n",
        "    \"\"\"\n",
        "    #Extraire les données de Macron\n",
        "    train_features = train_data[train_data['Nom']=='MACRON'].copy()\n",
        "    test_features = test_data.copy()\n",
        "\n",
        "    # harmoniser les colonnes d'identification\n",
        "    train_features['CodeINSEE'] = train_features['CodeINSEE'].astype(str).str.zfill(5)\n",
        "    test_features['CodeINSEE'] = test_features['CodeINSEE'].astype(str).str.zfill(5)\n",
        "\n",
        "    for i , (df , id_col) in enumerate(merge_list):\n",
        "        # Vérifier si la colonne d'identification est présente dans le DataFrame\n",
        "        if id_col not in df.columns:\n",
        "            raise ValueError(f\"La colonne d'identification '{id_col}' n'est pas présente dans le DataFrame à l'index {i}.\")\n",
        "\n",
        "        # Assurer que la colonne d'identification est au format string avec padding\n",
        "        df[id_col] = df[id_col].astype(str).str.zfill(5)\n",
        "\n",
        "        # nombre d'échantillons avant la fusion\n",
        "        n_samples_before = train_features.shape[0]\n",
        "\n",
        "        # Fusionner les DataFrames sur la colonne d'identification\n",
        "        train_features = pd.merge(train_features, df, left_on=base_id, right_on=id_col, how='left')\n",
        "\n",
        "        test_features = pd.merge(test_features, df, left_on=base_id, right_on=id_col, how='left')\n",
        "\n",
        "        # nombre d'échantillons après la fusion\n",
        "        n_samples_after = train_features.shape[0]\n",
        "\n",
        "        assert n_samples_before == n_samples_after, f\"Erreur de fusion : le nombre d'échantillons a changé après la fusion avec {id_col}.\"\n",
        "\n",
        "        # supprimer l'identifiant\n",
        "        if id_col != base_id:\n",
        "            train_features = train_features.drop(columns=[id_col], axis=1, errors='ignore')\n",
        "            test_features = test_features.drop(columns=[id_col], axis=1, errors='ignore')\n",
        "    #-----------------------------------------------------------------\n",
        "    # Supprimer les colonnes non informatives\n",
        "    train_features = train_features.drop(columns=cols_to_drop, axis=1, errors='ignore')\n",
        "    test_features = test_features.drop(columns=cols_to_drop, axis=1, errors='ignore')\n",
        "    #-----------------------------------------------------------------\n",
        "    # Supprimer les colonnes d'identification\n",
        "    train_features = train_features.drop(columns=IDs, axis=1 , errors='ignore')\n",
        "\n",
        "    if verbose :\n",
        "        missing1 = ((train_data.isnull().sum()/ train_data.shape[0]).sum()/train_data.shape[1] )* 100\n",
        "        missing2 = ((train_features.isnull().sum()/ train_features.shape[0]).sum()/train_features.shape[1] )* 100\n",
        "        #-------------------------------------------------------------\n",
        "        print(f\" {train_features.shape[1]} colonnes vs {train_data.shape[1]} avant la fusion\\n\")\n",
        "        print(f\" {len(cols_to_drop) + len(merge_list) + 1} colonnes supprimées pendant la fusion \\n\")\n",
        "        print(f\" \\n{train_features.shape[0]} lignes vs {train_data.shape[0]} avant la fusion\\n\")\n",
        "        print(f\" \\n{missing2}% de valeurs manquantes vs {missing1}% avant la fusion\")\n",
        "\n",
        "\n",
        "    return train_features, test_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMpEuUhHSybW"
      },
      "outputs": [],
      "source": [
        "train_data , test_data = prepare_datasets(res_train_df, res_test_df,\n",
        "                                          merge_list,\n",
        "                                          base_id = id_train,\n",
        "                                          cols_to_drop = cols_to_drop,\n",
        "                                          verbose=True)\n",
        "\n",
        "print(f\"forme du train_data : {train_data.shape}\")\n",
        "print(f\"forme du test_data : {test_data.shape}\")\n",
        "test_data.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Le_B_fFUl9"
      },
      "outputs": [],
      "source": [
        "X = train_data.drop(columns=[target], axis=1,errors='ignore')\n",
        "y = train_data[target]\n",
        "\n",
        "print(X.info())\n",
        "print(f\"forme des prédicteurs : {X.shape}\")\n",
        "print(f\"forme de la cible : {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BUCAoqTUG8R"
      },
      "source": [
        "# 2. Prétraitement , Pipeline et Selection automatique de features\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxuWfETcoCUH"
      },
      "source": [
        "## 2.1 Création du pipelines de prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0FCTwMrSybX"
      },
      "outputs": [],
      "source": [
        "def get_preprocessor(data,features=None):\n",
        "    \"\"\"\n",
        "    Crée un préprocesseur complet avec gestion des valeurs infinies, manquantes et aberrantes\n",
        "    \"\"\"\n",
        "    X=data.copy()\n",
        "    # Remplacer les valeurs infinies par NaN\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    if features is not None:\n",
        "        X = X[features]\n",
        "\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64',np.number]).columns\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category','bool']).columns\n",
        "\n",
        "    # conversion des colonnes catégorielles en type str pour éviter les type mixte\n",
        "    X[categorical_features] = X[categorical_features].astype(str)\n",
        "\n",
        "\n",
        "    # Pipeline pour les variables numériques\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', RobustScaler())\n",
        "    ] ,   memory='cache_directory')\n",
        "\n",
        "    # Pipeline pour les variables catégorielles\n",
        "    categorical_transformer = Pipeline(\n",
        "        steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder( sparse_output=False, handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "    # Combinaison des transformateurs\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder = 'drop',\n",
        "        verbose_feature_names_out=False,\n",
        "        n_jobs=-1)\n",
        "\n",
        "    return preprocessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaBiPt7-SybX"
      },
      "source": [
        "## 2.2 Selection automatique des features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZRK7DBRSybY"
      },
      "source": [
        "### 2.2.1 Selection grossière automatique avec xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7qls90yzQNi"
      },
      "outputs": [],
      "source": [
        "def remove_quasi_constant_features(df, threshold=0.01):\n",
        "    \"\"\"\n",
        "    Supprime les colonnes avec une variance inférieure à un seuil donné : elle sont quasi contantes\"\"\"\n",
        "\n",
        "    # Traitement des variables numériques\n",
        "    numerical_df = df.select_dtypes(include=[np.number, 'bool'])\n",
        "    variances = numerical_df.var()\n",
        "    low_variance_cols = variances[variances < threshold].index.tolist()\n",
        "\n",
        "    # Traitement des variables catégorielles (en regardant le ratio du mode)\n",
        "    categorical_df = df.select_dtypes(include=['object', 'category'])\n",
        "    cat_low_variance = []\n",
        "\n",
        "    for col in categorical_df.columns:\n",
        "        # Calcul du ratio de la valeur la plus fréquente\n",
        "        value_counts = categorical_df[col].value_counts(normalize=True)\n",
        "        if len(value_counts) > 0 and value_counts.iloc[0] > 1 - threshold:\n",
        "            cat_low_variance.append(col)\n",
        "\n",
        "    # Combinaison des résultats\n",
        "    all_low_variance = low_variance_cols + cat_low_variance\n",
        "\n",
        "    keep = sub(df.columns.tolist(), all_low_variance)\n",
        "\n",
        "    return all_low_variance, keep\n",
        "\n",
        "drop , keep = remove_quasi_constant_features(X)\n",
        "print(f\"nombre de colonnes jugées quasi constantes : {len(drop)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us0wqA-CzQNi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fast_feature_selection(x_data, y_data, n_features=20,sort=False):\n",
        "    \"\"\"\n",
        "    Selection automatique des ``n_features`` meilleures features avec  XGBoost\n",
        "    Args:\n",
        "        X (pd.DataFrame): DataFrame contenant les caractéristiques\n",
        "        y (pd.Series): Série contenant la cible\n",
        "        n_features (int): Nombre de features/colonnes à sélectionner\n",
        "        sort (bool): Si True, trie les features sélectionnées par importance décroissante\n",
        "    Returns:\n",
        "        list: Liste des noms des features sélectionnées\n",
        "    \"\"\"\n",
        "    # Créer une copie de X pour éviter les modifications sur l'original\n",
        "    X = x_data.copy()\n",
        "    # Supprimer les colonnes avec une variance inférieure à 0.01\n",
        "    drop, keep = remove_quasi_constant_features(X)\n",
        "\n",
        "    X = X[keep]\n",
        "    # Remplacer les valeurs infinies par NaN\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # identification des colonnes numériques et catégorielles\n",
        "    num_col = X.select_dtypes(include=[np.number,'float64','int64']).columns\n",
        "    cat_col = X.select_dtypes(include=['object', 'category','bool']).columns\n",
        "\n",
        "    # transformer tous les colonnes catégorielles en type str\n",
        "    # pour éviter les types mixtes\n",
        "    X[cat_col] = X[cat_col].astype(str)\n",
        "\n",
        "    # Préprocessing light\n",
        "    preprocessor = make_column_transformer(\n",
        "        (SimpleImputer(strategy='median'), num_col),\n",
        "        (Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "            ])              ,            cat_col ),\n",
        "        remainder='drop',\n",
        "        verbose_feature_names_out=False,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Modèle unique mais robuste\n",
        "    model = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('selector', SelectFromModel(\n",
        "            XGBRegressor(\n",
        "                tree_method='hist',\n",
        "                n_estimators=50,\n",
        "                max_depth=6),\n",
        "            max_features=n_features\n",
        "        )),\n",
        "        ('estimator', XGBRegressor())\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y_data)\n",
        "\n",
        "    transformed_features_names = model.named_steps['preprocessor'].get_feature_names_out(input_features=keep)\n",
        "\n",
        "    if sort:\n",
        "\n",
        "        importances = model.named_steps['estimator'].feature_importances_\n",
        "\n",
        "        feature_importance = list(zip(transformed_features_names, importances))\n",
        "\n",
        "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        selected_features = [name for name, _ in feature_importance[:n_features]]\n",
        "\n",
        "    selected_indices = model.named_steps['selector'].get_support(indices=True)\n",
        "\n",
        "    selected_features = [transformed_features_names[i] for i in selected_indices]\n",
        "\n",
        "    return list(selected_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppYd2pohzQNi"
      },
      "outputs": [],
      "source": [
        "def combined_feature_selection(X_data, y, n_features=20):\n",
        "    X = X_data.copy()\n",
        "    drop, keep = remove_quasi_constant_features(X)\n",
        "    X = X[keep]\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    num_col = X.select_dtypes(include=[np.number, 'float64','int64']).columns\n",
        "    cat_col = X.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "\n",
        "    X[cat_col] = X[cat_col].astype(str)\n",
        "\n",
        "    num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', RobustScaler())\n",
        "    ])\n",
        "\n",
        "    cat_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = make_column_transformer(\n",
        "        (num_pipeline, num_col),\n",
        "        (cat_pipeline, cat_col),\n",
        "        remainder='drop',\n",
        "        verbose_feature_names_out=False,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Apply preprocessing\n",
        "    X_transformed = preprocessor.fit_transform(X)\n",
        "    feature_names = preprocessor.get_feature_names_out(input_features=keep)\n",
        "\n",
        "    # Define models\n",
        "    xgb = XGBRegressor(tree_method='hist',\n",
        "                       n_estimators=50,\n",
        "                       max_depth=6,\n",
        "                       max_features=n_features,\n",
        "                       random_state=RANDOM_STATE)\n",
        "    lasso = LassoCV(cv=5)\n",
        "    rf = RandomForestRegressor(n_estimators=50,\n",
        "                               max_depth=6,\n",
        "                               max_features=n_features,\n",
        "                               random_state=RANDOM_STATE)\n",
        "\n",
        "    # Fit models\n",
        "    xgb.fit(X_transformed, y)\n",
        "    lasso.fit(X_transformed, y)\n",
        "    rf.fit(X_transformed, y)\n",
        "\n",
        "    # Get importances or coefficients\n",
        "    xgb_importance = xgb.feature_importances_\n",
        "    lasso_coef = np.abs(lasso.coef_)\n",
        "    rf_importance = rf.feature_importances_\n",
        "\n",
        "    # Normalize\n",
        "    def normalize(arr):\n",
        "        return arr / np.max(arr) if np.max(arr) != 0 else arr\n",
        "\n",
        "    # scores nomalisés\n",
        "    scores_norm = [\n",
        "        normalize(xgb_importance),\n",
        "        normalize(lasso_coef) ,\n",
        "        normalize(rf_importance)\n",
        "    ]\n",
        "    scores = np.vstack(scores_norm)\n",
        "    scores = np.median(scores, axis=0)\n",
        "\n",
        "    # Select top features\n",
        "    top_indices = np.argsort(scores)[::-1][:n_features]\n",
        "    selected_features = [feature_names[i] for i in top_indices]\n",
        "\n",
        "    return selected_features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3rVcwp_zQNj"
      },
      "outputs": [],
      "source": [
        "\n",
        "selected_features = fast_feature_selection(X, y, n_features=40)\n",
        "with open(\"40_best_features.json\" , 'w') as file:\n",
        "    json.dump(selected_features , file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4KM4qZ1zQNj"
      },
      "outputs": [],
      "source": [
        "for f in selected_features :\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJaRizJLzQNj"
      },
      "source": [
        "### 2.2.2 Selection plus fine pour un modèle donné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEagTsvyzQNj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_rmse(pipeline , X_sub, y,cv=None,\n",
        "                 message=\"Début du calcul...\",\n",
        "                 error=\"une erreur s'est produite\")->float:\n",
        "    \"\"\"\n",
        "    Function to compute cross-validated RMSE\n",
        "    \"\"\"\n",
        "    if cv is None:\n",
        "        cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "    if message is not None:\n",
        "        print(message)\n",
        "    try :\n",
        "        pipeline.fit(X_sub, y)\n",
        "        # Calculer le RMSE avec validation croisée\n",
        "        scores = cross_val_score(pipeline, X_sub, y,\n",
        "                                scoring='neg_root_mean_squared_error',\n",
        "                                cv=cv,n_jobs=-1)\n",
        "        return -scores.mean()\n",
        "    except Exception as e:\n",
        "        if error is not None:\n",
        "            print(error)\n",
        "            print(f\"Erreur : {e}\")\n",
        "        # Si une erreur se produit, on retourne une valeur élevée pour forcer la suppression de la feature\n",
        "        return float('inf')\n",
        "\n",
        "def make_pipeline(model , X,features=None):\n",
        "\n",
        "    preprocessor = get_preprocessor(X , features)\n",
        "    return Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "\n",
        "def backward_stepwise(model, X, y,\n",
        "                       initial_features:List[str]=None,\n",
        "                       min_features:int=3,\n",
        "                       rtol:float=0.005,\n",
        "                       max_degrad:float=0.005,\n",
        "                       cv=None,\n",
        "                       logg:bool=True)->Tuple[List[str] , float]:\n",
        "    \"\"\"\n",
        "    Backward stepwise feature selection algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The predictive model (must implement fit and predict).\n",
        "    - X: DataFrame of predictors.\n",
        "    - y: Target variable.\n",
        "    - initial_features: Initial subset of features to test. If None, uses all features.\n",
        "    - min_features: Minimum number of features to keep.\n",
        "    - max_degrad: Maximum allowed degradation in RMSE to consider a feature removable.\n",
        "    - rtol: Relative tolerance for improvement in RMSE. If improvement < rtol, stops early.\n",
        "\n",
        "    Returns:\n",
        "    - best_features: List of selected features.\n",
        "    - best_score: Best RMSE score achieved.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    all_features = X.columns.tolist()\n",
        "    if initial_features is None:\n",
        "        current_features = all_features.copy()\n",
        "    else:\n",
        "        current_features = [f for f in initial_features if f in all_features]\n",
        "\n",
        "    if min_features is None:\n",
        "        min_features = max(5, int(0.1 * len(current_features)))\n",
        "\n",
        "    if cv is None:\n",
        "        cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    pipeline = make_pipeline(model, X ,current_features)\n",
        "\n",
        "    # Compute initial RMSE\n",
        "    best_score = compute_rmse(pipeline,\n",
        "                              X[current_features], y,cv=cv , message=\"calcul du score initial...\" if logg else None,\n",
        "                              error=\"Erreur lors de l'évaluation initiale\" if logg else None)\n",
        "\n",
        "    if logg:\n",
        "        print(\"début des suppressions...\\nInitial RMSE: {best_score:.4f}\")\n",
        "    features_removed = []\n",
        "    improve = True\n",
        "    while improve and len(current_features) > min_features:\n",
        "        worst_feature = None\n",
        "        current_rmse = best_score\n",
        "        # essai de suppression de chaque feature\n",
        "        for feature in current_features:\n",
        "            trial_features = [f for f in current_features if f != feature]\n",
        "            trial_pipeline = make_pipeline(model, X,trial_features)\n",
        "            # Compute RMSE without the feature\n",
        "            trial_rmse = compute_rmse(trial_pipeline,\n",
        "                                      X[trial_features], y,cv=cv,message=f\"Test sans '{feature}'\"if logg else None,error=f\"Erreur en testant sans {feature} \"if logg else None)\n",
        "\n",
        "            relative_change = (best_score-trial_rmse) / best_score\n",
        "            # If removing the feature improves RMSE beyond rtol\n",
        "            if relative_change >= rtol and trial_rmse < current_rmse:\n",
        "                    current_rmse = trial_rmse\n",
        "                    worst_feature = feature\n",
        "            elif relative_change < -max_degrad:\n",
        "                if logg:\n",
        "                    print(f\" suppression de {feature} rejetée : dégradation de {-relative_change*100:.2f} %\")\n",
        "\n",
        "        # Remove the worst feature if found\n",
        "        if worst_feature is not None:\n",
        "            current_features.remove(worst_feature)\n",
        "            features_removed.append(worst_feature)\n",
        "            best_score = current_rmse\n",
        "            improve = True\n",
        "            if logg:\n",
        "                print(f\" '{worst_feature}' supprimée  , nouvelle RMSE: {best_score:.4f},  {len(current_features)} features restante\")\n",
        "        else:\n",
        "            improve = False\n",
        "\n",
        "    if logg:\n",
        "        print(f\"selction terminée {len(current_features)} Features conservées. RMSE final: {best_score:.4f}\")\n",
        "    return current_features, best_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfYL455YUG8R"
      },
      "source": [
        "# 3. MODELISATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgKBt6IzQNj"
      },
      "source": [
        "### Separation X_train , y_train ,...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D3QFz2ozQNk"
      },
      "outputs": [],
      "source": [
        "train_data , test_data = prepare_datasets(res_train_df, res_test_df,\n",
        "                                          merge_list,\n",
        "                                          base_id = id_train,\n",
        "                                          cols_to_drop = cols_to_drop,\n",
        "                                          verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehN7diT5zQNk"
      },
      "outputs": [],
      "source": [
        "X = train_data.drop(columns=[target], axis=1,errors='ignore')\n",
        "y = train_data[target]\n",
        "\n",
        "#selection des 20 meilleures features\n",
        "selected_features = fast_feature_selection(X, y, n_features=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrM_w4fdzQNk"
      },
      "outputs": [],
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(safe_feature_selection(X,selected_features),y, test_size=0.2, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNn6dadQzQNk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI3DWHygKc_u"
      },
      "source": [
        "### Fonction d'optimisation des hyperparamètres et des features pour un moèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4sT5oNtzQNk"
      },
      "outputs": [],
      "source": [
        "def quick_gridsearch(model, param_grid, X, y, cv=3, scoring='neg_root_mean_squared_error', verbose=0):\n",
        "    \"\"\"\n",
        "    Effectue une GridSearchCV rapide pour évaluer rapidement un modèle de régression.\n",
        "\n",
        "    Returns:\n",
        "    - best_estimator_: le modèle entraîné avec les meilleurs paramètres\n",
        "    - best_params_: dictionnaire des meilleurs paramètres\n",
        "    - best_score_: meilleur score trouvé (négatif car RMSE)\n",
        "    - cv_results_: résultats détaillés\n",
        "    \"\"\"\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid,\n",
        "        scoring=scoring,\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        verbose=verbose,\n",
        "        return_train_score=True\n",
        "    )\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_, grid_search.cv_results_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY6IaEcCneTu"
      },
      "outputs": [],
      "source": [
        "class TQDMCallback:\n",
        "    \"\"\"\n",
        "    Callback to update tqdm progress bar during hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    def __init__(self, total):\n",
        "        self.pbar = tqdm(total=total, desc=\"Optimisation des hyperparamètres\", unit=\"itération\")\n",
        "\n",
        "    def __call__(self, study, trial):\n",
        "        self.pbar.update(1)\n",
        "        self.pbar.set_postfix({\"Best RMSE\": study.best_value})\n",
        "\n",
        "#Fonction d'optimisation des hyperparamètres et des features pour un modèle\n",
        "def optimise_model(model, params, X, y,\n",
        "                   n_trials: int = 3,\n",
        "                   model_name: str = '',\n",
        "                   cv:int=3,\n",
        "                   selected_feature_fn = None\n",
        "                   ) -> Tuple[Dict, List[str], optuna.study.Study]:\n",
        "    \"\"\"\n",
        "    Optimise les hyperparamètres et selectionne les features optimales pour le modèle\n",
        "\n",
        "    Args\n",
        "    ---\n",
        "    model : (fit_Object)\n",
        "      instance du modèle choisit , ici ce sera soit Lasso , ElesticNet ,  XGboost , LightGBM\n",
        "    params :\n",
        "      disctionnaire permettant de definir l'espace des hyperparamètres à optimiser\n",
        "    X : (pd.DataFrame)\n",
        "      dataframes des predicteurs\n",
        "    y : (pd.DataFrame)\n",
        "      variables cible\n",
        "    n_trials : (int)\n",
        "      nombre d'essais pour l'optimisation des hyperparamètres\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Dict, List[str], optuna.study.Study]:\n",
        "      tuple contenant\n",
        "    -best_params : (dict)\n",
        "      dictionnaire des meilleurs hyperparamètres\n",
        "    -best_features : (list)\n",
        "      liste des features sélectionnées\n",
        "    -study : (optuna.study.Study)\n",
        "      instance de l'étude d'optimisation\n",
        "    \"\"\"\n",
        "    if cv is None:\n",
        "      cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE )\n",
        "\n",
        "    def objective(trial):\n",
        "        # Définir les espaces de recherche pour chaque hyperparamètre\n",
        "        param_grid = {}\n",
        "        for name, conf in params.items():\n",
        "            if conf['type'] == 'int':\n",
        "                if isinstance(conf['range'], (list, tuple)) and len(conf['range']) == 2:\n",
        "                    param_grid[name] = trial.suggest_int(name, conf['range'][0], conf['range'][1])\n",
        "                else:\n",
        "                    print(f\"Erreur: conf['range'] n'est pas une liste ou un tuple valide pour {name} (type int)\")\n",
        "                    continue  # Passe à l'hyperparamètre suivant\n",
        "            elif conf['type'] == 'float':\n",
        "                if isinstance(conf['range'], (list, tuple)) and len(conf['range']) == 2:\n",
        "                    param_grid[name] = trial.suggest_float(name, conf['range'][0], conf['range'][1])\n",
        "                else:\n",
        "                    print(f\"Erreur: conf['range'] n'est pas une liste ou un tuple valide pour {name} (type float)\")\n",
        "                    continue  # Passe à l'hyperparamètre suivant\n",
        "            elif conf['type'] == 'categorical':\n",
        "                if isinstance(conf['range'], (list, tuple)):\n",
        "                    param_grid[name] = trial.suggest_categorical(name, conf['range'])\n",
        "                else:\n",
        "                    print(f\"Erreur: conf['range'] n'est pas une liste ou un tuple valide pour {name} (type categorical)\")\n",
        "                    continue  # Passe à l'hyperparamètre suivant\n",
        "            elif conf['type'] == 'log':\n",
        "                if isinstance(conf['range'], (list, tuple)) and len(conf['range']) == 2:\n",
        "                    param_grid[name] = trial.suggest_float(name, conf['range'][0], conf['range'][1], log=True)\n",
        "                else:\n",
        "                    print(f\"Erreur: conf['range'] n'est pas une liste ou un tuple valide pour {name} (type log)\")\n",
        "                    continue  # Passe à l'hyperparamètre suivant\n",
        "\n",
        "        # Appliquer les paramètres au modèle\n",
        "        model_trial = clone(model).set_params(**param_grid)\n",
        "\n",
        "        try:\n",
        "            # Selection des features\n",
        "            if selected_feature_fn is not None:\n",
        "                selected_features = selected_feature_fn(X, y)\n",
        "            else:\n",
        "              selected_features = backward_stepwise(model_trial, X, y, cv=cv, logg=False)[0]\n",
        "\n",
        "            # sélection sécurisée des colonnes\n",
        "            X_selected = safe_feature_selection(X, selected_features)\n",
        "\n",
        "            # si aucune colonne valide , utiliser toutes le colonnes\n",
        "            if X_selected.shape[1] == 0:\n",
        "                X_selected = X.copy()\n",
        "                selected_features = X.columns.tolist()\n",
        "\n",
        "            prepro = get_preprocessor(X_selected)\n",
        "            pipeline = Pipeline([\n",
        "                ('preprocessor', prepro),\n",
        "                ('model', model_trial)\n",
        "            ])\n",
        "\n",
        "            trial.set_user_attr('selected_features', selected_features)\n",
        "\n",
        "            # Effectuer la validation croisée\n",
        "            scores = cross_val_score(pipeline,\n",
        "                                      X_selected, y,\n",
        "                                      cv=cv, scoring='neg_root_mean_squared_error',\n",
        "                                      n_jobs=-1)\n",
        "\n",
        "            score = -np.mean(scores)\n",
        "            trial.report(score, step=0)\n",
        "            if trial.should_prune():\n",
        "                raise TrialPruned()\n",
        "            print(f\"[Trial {trial.number}] RMSE: {score:.4f} | Params: {param_grid}\")\n",
        "            return score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Trial {trial.number} Erreur : {e}\")\n",
        "            # on retourne une valeur élevée pour forcer la suppression de la feature\n",
        "            trial.set_user_attr('error', str(e))\n",
        "            trial.set_user_attr('selected_features', X.columns.tolist())\n",
        "            return float('inf')\n",
        "\n",
        "    study = optuna.create_study(direction='minimize',\n",
        "                                  pruner=MedianPruner())\n",
        "    study.optimize(objective,\n",
        "                   n_trials=n_trials,\n",
        "                   n_jobs=-1,\n",
        "                   callbacks=[TQDMCallback(n_trials)])\n",
        "\n",
        "    joblib.dump(study, f'{model_name}_study.pkl')\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_features = study.best_trial.user_attrs['selected_features']\n",
        "\n",
        "    return best_params, best_features, study\n",
        "\n",
        "\n",
        "\n",
        "def optimise_then_select(model, param_space, X, y,\n",
        "                           n_trials=10, cv=None,\n",
        "                           min_features=5,\n",
        "                           rtol=0.01, logg=False,\n",
        "                           model_name=None):\n",
        "  best_params , _ , _ = optimise_model(model = model,\n",
        "                                       params=  param_space ,\n",
        "                                       X=X , y=y ,\n",
        "                                       selected_feature_fn=None,\n",
        "                                       n_trials=n_trials ,\n",
        "                                       model_name=model_name ,\n",
        "                                       cv=cv,\n",
        "                                       )\n",
        "  model = model.set_params(**best_params)\n",
        "\n",
        "  best_features = backward_stepwise(model=model,\n",
        "                                    X=X,\n",
        "                                    y=y,\n",
        "                                    min_features=min_features,\n",
        "                                    rtol=rtol,\n",
        "                                    logg=False)[0]\n",
        "  return model , best_features , best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WALDW_5wUG8R"
      },
      "source": [
        "## Initialisation des modèles et recherche d'hyperparamètres\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Définir les modèles et leurs espaces de recherche\n",
        "models_grids = {\n",
        "    \"Lasso\": (             # modèle imposé\n",
        "        Lasso(random_state=RANDOM_STATE),\n",
        "        {\n",
        "            'alpha': {'type': 'log', 'range': (1e-4, 10)},\n",
        "            'max_iter': {'type': 'int', 'range': (1000, 5000)},\n",
        "            'fit_intercept': {'type': 'categorical', 'range': [True, False]}\n",
        "        }\n",
        "    ),\n",
        "    \"ElasticNet\": (       #modèle au choix\n",
        "        ElasticNet(random_state=RANDOM_STATE),\n",
        "        {\n",
        "            'alpha': {'type': 'log', 'range': (1e-4, 10)},\n",
        "            'l1_ratio': {'type': 'float', 'range': (0.1, 1)},\n",
        "            'max_iter': {'type': 'int', 'range': (1000, 5000)},\n",
        "            'fit_intercept': {'type': 'categorical', 'range': [True, False]}\n",
        "        }\n",
        "    ),\n",
        "    \"XGBoost\": (      # modèle au choix 2\n",
        "        XGBRegressor(random_state=RANDOM_STATE),\n",
        "        {\n",
        "            'n_estimators': {'type': 'int', 'range': (100, 1000)},\n",
        "            'max_depth': {'type': 'int', 'range': (3, 10)},\n",
        "            'learning_rate': {'type': 'float', 'range': (0.01, 0.3)},\n",
        "            'subsample': {'type': 'float', 'range': (0.6, 1.0)},\n",
        "            'colsample_bytree': {'type': 'float', 'range': (0.6, 1.0)},\n",
        "            'gamma': {'type': 'float', 'range': (0, 5)},\n",
        "            'reg_alpha': {'type': 'float', 'range': (0, 1)},\n",
        "            'reg_lambda': {'type': 'float', 'range': (0, 1)}\n",
        "        }\n",
        "    )\n",
        "}\n",
        "\n",
        "# 2. Boucle d'optimisation + sélection des best features\n",
        "models_info = {}\n",
        "\n",
        "for name, (model, space) in models_grids.items():\n",
        "    print(f\"\\n🔍 Traitement du modèle : {name}\")\n",
        "\n",
        "    final_model, selected_features, best_params = optimise_then_select(\n",
        "        model=model,\n",
        "        param_space=space,\n",
        "        X=X_train,\n",
        "        y=y_train,\n",
        "        n_trials=200,\n",
        "        min_features=5,\n",
        "        rtol=0.01,\n",
        "        logg=False,\n",
        "        model_name = name\n",
        "    )\n",
        "\n",
        "    models_info[name] = {\n",
        "        \"model\": final_model,\n",
        "        \"features\": selected_features,\n",
        "        \"params\": best_params\n",
        "    }\n"
      ],
      "metadata": {
        "id": "fJEa5tai1Ib8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MITTYAIWUG8T"
      },
      "source": [
        "## Comparaison des modèles et selection du meilleur modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxPe-uLzQNl"
      },
      "source": [
        "###   Comparaison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9O8mnP8zQNl"
      },
      "outputs": [],
      "source": [
        "def compare_grid_models(models_grids: Dict[str, Tuple], X_train: pd.DataFrame, y_train: pd.Series, cv: int = 3) -> Tuple[pd.DataFrame, Dict[str, Pipeline]]:\n",
        "    \"\"\"\n",
        "    Compare plusieurs modèles de régression à l'aide de GridSearchCV rapide.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    - models_grids : dict où chaque clé est le nom du modèle et la valeur est un tuple (modèle sklearn, grille d'hyperparamètres)\n",
        "    - X_train : données d'entraînement\n",
        "    - cv : nombre de folds pour la validation croisée\n",
        "\n",
        "    Returns:\n",
        "    - results_df: DataFrame des scores RMSE moyens\n",
        "    - best_models: dictionnaire des pipelines optimaux entraînés\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    best_models = {}\n",
        "\n",
        "    for name, (model, grid) in models_grids.items():\n",
        "        print(f\"\\n Recherche GridSearch pour {name}...\")\n",
        "\n",
        "        # Prétraitement et pipeline\n",
        "        preprocessor = get_preprocessor(X_train)\n",
        "        pipeline = Pipeline([\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\"model\", model)\n",
        "        ])\n",
        "\n",
        "        # Adapter la grille avec le préfixe \"model__\"\n",
        "        param_grid = {'model__' + k: v for k, v in grid.items()}\n",
        "\n",
        "        best_pipeline, best_params, best_score, _ = quick_gridsearch(\n",
        "            pipeline,\n",
        "            param_grid=param_grid,\n",
        "            X=X_train,\n",
        "            y=y_train,\n",
        "            cv=cv,\n",
        "            scoring='neg_root_mean_squared_error',\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": name,\n",
        "            \"Best RMSE (CV)\": -best_score,\n",
        "            \"Best Params\": best_params\n",
        "        })\n",
        "        best_models[name] = best_pipeline\n",
        "\n",
        "    results_df = pd.DataFrame(results).sort_values(by=\"Best RMSE (CV)\")\n",
        "    return results_df, best_models\n",
        "\n",
        "\n",
        "\n",
        "results_df, pipelines = compare_grid_models(models_grids, X_train, y_train)\n",
        "print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmAKMUKBzQNm"
      },
      "source": [
        "### Selection automatique du meilleur Modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci0soYcozQNm"
      },
      "outputs": [],
      "source": [
        "# selection du meilleur modèle\n",
        "\n",
        "best_model_name = results_df.iloc[0]['Model'] # nom du meilleur modèle\n",
        "\n",
        "print(f\"le meilleur modèle est {best_model_name}\")\n",
        "best_pipeline = best_models[best_model_name] #extraction de la meilleur  pipeline\n",
        "best_features = models_info[best_model_name]['features'] #extraction des features optimal\n",
        "best_model = models_info[best_model_name]['model'] #extraction du meilleur modèle\n",
        "best_params = models_info[best_model_name]['params'] #extraction des meilleurs paramètres\n",
        "\n",
        "# entrainement du meilleur modèle\n",
        "final_best_model = best_pipeline.fit(X_train[best_features], y_train)\n",
        "final_best_features = best_features\n",
        "\n",
        "# mise à jour de models_info:\n",
        "for name  in models_info:\n",
        "  models_info[name][\"is_best\"] = (name == best_model_name)\n",
        "for name , pipe in pipelines:\n",
        "  models_info[name][\"pipeline\"] = pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_QDqxFHzQNm"
      },
      "source": [
        "##  Quelques Analyses graphiques des modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2FQMAkjzQNm"
      },
      "outputs": [],
      "source": [
        "def analyze_model_predictions(y_true, y_pred, model_name=\"\"):\n",
        "    \"\"\"\n",
        "    Génère des graphiques d'analyse des performances d'un modèle de régression :\n",
        "    - Résidus vs Prédictions\n",
        "    - Histogramme des résidus\n",
        "    - QQ-plot\n",
        "    - Vraies vs Prédictions\n",
        "    - Affichage des métriques\n",
        "\n",
        "    Args:\n",
        "    - y_true: valeurs réelles\n",
        "    - y_pred: prédictions du modèle\n",
        "    - model_name: nom du modèle (affiché sur les titres)\n",
        "    \"\"\"\n",
        "    residuals = y_true - y_pred\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n📊 Évaluation du {model_name}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"R²  : {r2:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Résidus vs Prédictions\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.scatterplot(x=y_pred, y=residuals)\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel(\"Prédictions\")\n",
        "    plt.ylabel(\"Résidus\")\n",
        "    plt.title(\"Résidus vs Prédictions\")\n",
        "\n",
        "    # Histogramme des résidus\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.histplot(residuals, kde=True)\n",
        "    plt.title(\"Distribution des résidus\")\n",
        "\n",
        "    # QQ-plot\n",
        "    plt.subplot(2, 2, 3)\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "    plt.title(\"QQ-Plot des résidus\")\n",
        "\n",
        "    # Vraies vs Prédictions\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.scatterplot(x=y_true, y=y_pred)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "    plt.xlabel(\"Vraies valeurs\")\n",
        "    plt.ylabel(\"Prédictions\")\n",
        "    plt.title(\"Vraies vs Prédictions\")\n",
        "\n",
        "    plt.suptitle(f\"Analyse du {model_name}\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.savefig(f\"graphiques d'analyse du modèle {model_name}\")\n",
        "    plt.show()\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def plot_feature_importance(model, feature_names, model_name=\"\"):\n",
        "    \"\"\"\n",
        "    Trace l'importance des features pour un modèle donné.\n",
        "    \"\"\"\n",
        "    if hasattr(model, \"coef_\"):\n",
        "        importances = model.coef_\n",
        "    elif hasattr(model, \"feature_importances_\"):\n",
        "        importances = model.feature_importances_\n",
        "    else:\n",
        "        print(f\"Aucune importance de feature disponible pour {model_name}\")\n",
        "        return\n",
        "\n",
        "    sorted_idx = np.argsort(importances)[::-1]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=np.abs(importances)[sorted_idx], y=np.array(feature_names)[sorted_idx])\n",
        "    plt.title(f\"Importance des variables - {model_name}\")\n",
        "    plt.xlabel(\"Importance absolue\")\n",
        "    plt.ylabel(\"Variables\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"Figures/importances_{model_name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "#----------------------------------------------------------------\n",
        "def analyze_all_models(models_info , X_test, y_test):\n",
        "    \"\"\"\n",
        "    Applique l'analyse des prédictions à tous les modèles comparés.\n",
        "    \"\"\"\n",
        "    for model_name, info in models_info.items():\n",
        "        features = info.get(\"features\" , X_test.columns.tolist())\n",
        "        pipeline = info.get(\"pipeline\")\n",
        "        x_val = safe_feature_selection(X_test, features) if features else X_test\n",
        "\n",
        "        y_pred = pipeline.predict(X_test[features])\n",
        "        analyze_model_predictions(y_test, y_pred, model_name)\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "os.makedirs(\"Figures\",exist_ok=True)\n",
        "\n",
        "# tracer les graphiques pour les analyses des trois modèles\n",
        "analyze_all_models(models_info , X_test, y_test)\n",
        "\n",
        "\n",
        "# graphique de l'importance des features pour le meilleur modèle\n",
        "plot_feature_importance(final_best_model , final_best_features, model_name=best_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVyB8t09bGp5"
      },
      "source": [
        "# 4. Soumission sur Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgo0nz3BxuUW"
      },
      "outputs": [],
      "source": [
        "# Soumission sur Kaggle\n",
        "\n",
        "# copier le fichier de test déja merge\n",
        "data_test = test_data.copy()\n",
        "\n",
        "# selectionner les features optimaux pour le best_model\n",
        "data_test_submit = data_test[final_best_features]\n",
        "\n",
        "# Calcul des prédictions sur le dataset de test final\n",
        "y_pred_test = final_best_model.predict(data_test_submit)\n",
        "\n",
        "# Ajout des prédictions au dataframe\n",
        "data_test['Prediction'] = y_pred_test\n",
        "\n",
        "submission = data_test[['CodeINSEE', 'Prediction']]\n",
        "\n",
        "# sauvegarder la liste des features\n",
        "with open(f\"final_best_features_with_{best_model_name}.json\" , 'w') as file:\n",
        "    json.dump(final_best_features , file)\n",
        "\n",
        "# sauvegarder les prédictions\n",
        "submission.to_csv('results_test_predicted.csv', index=False)\n",
        "print(f\"\\nFichier de soumission 'results_test_predicted.csv' généré avec succès \\nForme {submission.shape}  \")\n",
        "\n",
        "submission.head(5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "74V_oeHHUG8N",
        "l86laxxtXd-J",
        "0VJxyPKWSya9",
        "f7CvwbxZSya9",
        "jCiAJj37UG8P",
        "ww_frSK5neTo",
        "ExVYdARYuZTa",
        "8F66BiGGSybV",
        "aHi6J682SybV",
        "HI3DWHygKc_u",
        "N2voDkixUG8S",
        "G1dFnY3-UG8S",
        "tBdP40c9UG8S",
        "tto_G2-OUG8S",
        "9QbuzZxxeJXd",
        "TJvbczlpUG8S",
        "MD5Hc_XgUG8S",
        "5_i1RctpUG8S",
        "awxpmlMoUG8S",
        "rRQ6BeBLrf6k",
        "MITTYAIWUG8T",
        "lVyB8t09bGp5"
      ],
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}